diff --git a/bad_mp3_ids.txt b/bad_mp3_ids.txt
index 82a7731..a0f774f 100644
--- a/bad_mp3_ids.txt
+++ b/bad_mp3_ids.txt
@@ -3,3 +3,4 @@
 72870	LibsndfileError	Error opening 'AudioCaps_mp3/train/72870.mp3': File does not exist or is not a regular file (possibly a pipe?).
 24172	LibsndfileError	Error opening 'AudioCaps_mp3/train/24172.mp3': File does not exist or is not a regular file (possibly a pipe?).
 24172	LibsndfileError	Error opening 'AudioCaps_mp3/train/24172.mp3': File does not exist or is not a regular file (possibly a pipe?).
+77971	LibsndfileError	Error opening 'AudioCaps_mp3/train/77971.mp3': File does not exist or is not a regular file (possibly a pipe?).
diff --git a/config.yaml b/config.yaml
index 008b1cb..d38c37c 100644
--- a/config.yaml
+++ b/config.yaml
@@ -16,12 +16,12 @@ val_index_csv:    Spatial_AudioCaps/takamichi09/for_delsa_spatialAudio/val_preco
 val_batch_size: 64
 
 audio_base: AudioCaps_mp3 
-batch_size: 8
-n_views: 8
+batch_size: 24
+n_views: 3
 val_n_views: 24
 epochs: 50
 lr: 0.0001
 device: auto            # "auto" → cuda if available
 wandb: true
 proj: delsa-sup-contrast
-run_name: None
\ No newline at end of file
+run_name: None
diff --git a/dataset/__pycache__/audio_rir_dataset.cpython-312.pyc b/dataset/__pycache__/audio_rir_dataset.cpython-312.pyc
index 363972c..944a9a6 100644
Binary files a/dataset/__pycache__/audio_rir_dataset.cpython-312.pyc and b/dataset/__pycache__/audio_rir_dataset.cpython-312.pyc differ
diff --git a/dataset/__pycache__/precomputed_val_dataset.cpython-312.pyc b/dataset/__pycache__/precomputed_val_dataset.cpython-312.pyc
index 24fa79d..8eff6a3 100644
Binary files a/dataset/__pycache__/precomputed_val_dataset.cpython-312.pyc and b/dataset/__pycache__/precomputed_val_dataset.cpython-312.pyc differ
diff --git a/dataset/audio_rir_dataset.py b/dataset/audio_rir_dataset.py
index db3ae5d..27a21ec 100644
--- a/dataset/audio_rir_dataset.py
+++ b/dataset/audio_rir_dataset.py
@@ -98,7 +98,8 @@ class AudioRIRDataset(Dataset):
                  batch_size : int | None = None,
                  stats_path: str = "/home/takamichi-lab-pc09/DELSA/RIR_dataset/stats.pt",
                  hop: int =100,
-                 bad_log_path: str | None = "bad_mp3_ids.txt"):
+                 bad_log_path: str | None = "bad_mp3_ids.txt",
+                 defer_convolution: bool = True):
         super().__init__()
         # ── 設定ロード ──
 
@@ -112,6 +113,7 @@ class AudioRIRDataset(Dataset):
         self.n_fft = n_fft
         self.hop = hop
         self.foa_len = int(FOA_SR*MAX_DURATION_SEC)
+        self.defer_convolution = defer_convolution
         # audio_csv読み込み
         df = pd.read_csv(csv_audio)
         df = df[df["audiocap_id"].apply(
@@ -212,29 +214,45 @@ class AudioRIRDataset(Dataset):
             rir_paths = random.sample(self.rir_paths, k=self.n_views)
 
         for rir_path in rir_paths:
-            wet = self._apply_rir(dry, rir_path) #[4, T10]   
-            #ToDo済: A-format to B-format　　　Spatial_AudioCaps/scripts/SpatialAudio.pyを参考に
-             #ToDo済: captionも空間拡張する(ルールべースの書き換え)
-            meta = self.rir_meta[rir_path].copy()      # シャローコピーで安全に複製 :contentReference[oaicite:5]{index=5}
-            meta["area_m2_norm"]  = (meta["area_m2"]           - self.area_mean) / self.area_std
-            meta["distance_norm"] = (meta["source_distance_m"] - self.dist_mean) / self.dist_std
-            meta["t30_norm"]      = (meta["fullband_T30_ms"]   - self.t30_mean)  / self.t30_std
-            azimuth = meta["azimuth_deg"]
-            elevation = meta["elevation_deg"]
-            direction_vec = torch.tensor(
-                [np.deg2rad(azimuth), np.deg2rad(elevation)],
-                dtype=torch.float32
-            )
-            meta["direction_vec"] = direction_vec
-            caption_spatial = rewrite_caption(caption, meta)
-            omni_48k = wet[0]  # [T10]
-            
-            # 16kにリサンプリング
-            wet_16k = torchaudio.functional.resample(wet, orig_freq=FOA_SR, new_freq=IV_SR)
-            i_act, i_rea = foa_to_iv(wet_16k.unsqueeze(0), n_fft=self.n_fft, hop=self.hop)
-            i_act, i_rea = i_act.squeeze(0), i_rea.squeeze(0)
+            if self.defer_convolution:
+                audio_features_list.append({"dry": dry, "rir_path": rir_path}) #[4, T10]
+                meta = self.rir_meta[rir_path].copy()      # シャローコピーで安全に複製 :contentReference[oaicite:5]{index=5}
+                meta["area_m2_norm"]  = (meta["area_m2"]           - self.area_mean) / self.area_std
+                meta["distance_norm"] = (meta["source_distance_m"] - self.dist_mean) / self.dist_std
+                meta["t30_norm"]      = (meta["fullband_T30_ms"]   - self.t30_mean)  / self.t30_std
+                azimuth = meta["azimuth_deg"]
+                elevation = meta["elevation_deg"]
+                direction_vec = torch.tensor(
+                    [np.deg2rad(azimuth), np.deg2rad(elevation)],
+                    dtype=torch.float32
+                )
+                meta["direction_vec"] = direction_vec
+                caption_spatial = rewrite_caption(caption, meta)
+
+            else: 
+                wet = self._apply_rir(dry, rir_path) #[4, T10]
+                #ToDo済: A-format to B-format　　　Spatial_AudioCaps/scripts/SpatialAudio.pyを参考に
+                #ToDo済: captionも空間拡張する(ルールべースの書き換え)
+                meta = self.rir_meta[rir_path].copy()      # シャローコピーで安全に複製 :contentReference[oaicite:5]{index=5}
+                meta["area_m2_norm"]  = (meta["area_m2"]           - self.area_mean) / self.area_std
+                meta["distance_norm"] = (meta["source_distance_m"] - self.dist_mean) / self.dist_std
+                meta["t30_norm"]      = (meta["fullband_T30_ms"]   - self.t30_mean)  / self.t30_std
+                azimuth = meta["azimuth_deg"]
+                elevation = meta["elevation_deg"]
+                direction_vec = torch.tensor(
+                    [np.deg2rad(azimuth), np.deg2rad(elevation)],
+                    dtype=torch.float32
+                )
+                meta["direction_vec"] = direction_vec
+                caption_spatial = rewrite_caption(caption, meta)
+                omni_48k = wet[0]  # [T10]
+                
+                # 16kにリサンプリング
+                wet_16k = torchaudio.functional.resample(wet, orig_freq=FOA_SR, new_freq=IV_SR)
+                i_act, i_rea = foa_to_iv(wet_16k.unsqueeze(0), n_fft=self.n_fft, hop=self.hop)
+                i_act, i_rea = i_act.squeeze(0), i_rea.squeeze(0)
 
-            audio_features_list.append({"i_act": i_act, "i_rea": i_rea, "omni_48k": omni_48k})
+                audio_features_list.append({"i_act": i_act, "i_rea": i_rea, "omni_48k": omni_48k})
             src_ids.append(self.source_map[idx])
             spa_ids.append(self.space_map[rir_path])
             texts.append(caption_spatial)
@@ -268,6 +286,7 @@ def collate_fn(batch):
     meta_keys = rir_meta_list[0].keys()
     rir_meta_dict = {}
     for key in meta_keys:
+        
         vals = [m[key] for m in rir_meta_list]
         first = vals[0]
 
diff --git a/model/__pycache__/delsa_model.cpython-312.pyc b/model/__pycache__/delsa_model.cpython-312.pyc
index a2c4f5b..7634d8c 100644
Binary files a/model/__pycache__/delsa_model.cpython-312.pyc and b/model/__pycache__/delsa_model.cpython-312.pyc differ
diff --git a/others/precompute_val.py b/others/precompute_val.py
index bb5f5be..b75ce44 100644
--- a/others/precompute_val.py
+++ b/others/precompute_val.py
@@ -178,3 +178,6 @@ if __name__ == "__main__":
     p.add_argument("--out_dir", help="Output root (default: data/val_precomputed)")
     p.add_argument("--n_views", type=int, help="<=0 なら RIR 全部を使用（デフォルトは config.yaml の n_views）")
     main(p.parse_args())
+
+
+
diff --git a/train_ddp.py b/train_ddp.py
index 536e967..a5acbb8 100644
--- a/train_ddp.py
+++ b/train_ddp.py
@@ -79,23 +79,54 @@ def load_config(path: str | None = None) -> dict:
     return cfg
 
 def sup_contrast(a, b, labels, logit_scale, eps=1e-8, *, symmetric=True, exclude_diag=False):
+    """
+    Traditional Supervised Contrastive (Khosla et al., 2020) に準拠した2塔版。
+    - 同ラベルを正例。2塔なので a のアンカーに対して b 側の同ラベル全てが正例。
+    - exclude_diag=True のときは、対角 (i,i) を【分子・分母の両方】から外す（完全一貫）。
+      False のときは対角を分子・分母の両方に含める（同一インスタンス正例を含める）。
+    """
     a = F.normalize(a, dim=1); b = F.normalize(b, dim=1)
-    #scale = torch.clamp(logit_scale, max=math.log(1e2)).exp()
-    logits_t2a = (a @ b.T) * logit_scale
+
+    # CLIP式のlogスケールを渡しているならこちらを使う：
+    # scale = torch.clamp(logit_scale, max=math.log(1e2)).exp()
+    # そうでなければ logit_scale は線形スカラーとして渡す
+    scale = logit_scale
+
+    logits_t2a = (a @ b.T) * scale
     logits_a2t = logits_t2a.T if symmetric else None
 
+    labels = labels.view(-1)
+
     def _dir_loss(logits):
         B = logits.size(0)
-        pos_mask = labels[:, None].eq(labels[None, :])
-        max_sim, _ = logits.max(dim=1, keepdim=True)
-        logits = logits - max_sim.detach()
-        diag_mask = torch.eye(B, dtype=torch.bool, device=logits.device) if exclude_diag else torch.zeros(B,B,dtype=torch.bool, device=logits.device)
-        exp_sim = torch.exp(logits) * (~diag_mask)
-        denom = exp_sim.sum(dim=1, keepdim=True) + eps
+
+        # 正例マスク（同ラベル）。2塔なので「自分自身のベクトル」は存在しない。
+        pos_mask = labels[:, None].eq(labels[None, :]).float()
+
+        # 数値安定化
+        logits = logits - logits.max(dim=1, keepdim=True).values.detach()
+
+        # 分母マスク：exclude_diag によって対角を分子・分母で一貫処理
+        if exclude_diag:
+            pos_mask = pos_mask.clone()
+            pos_mask.fill_diagonal_(0.0)  # 分子からも対角を除外（完全一貫）
+            denom_mask = (~torch.eye(B, dtype=torch.bool, device=logits.device)).float()
+        else:
+            denom_mask = torch.ones_like(pos_mask)
+
+        exp_logits = torch.exp(logits) * denom_mask
+        denom = exp_logits.sum(dim=1, keepdim=True) + eps
         log_prob = logits - denom.log()
-        mean_log_pos = (log_prob * pos_mask.float()).sum(dim=1) / pos_mask.sum(dim=1).clamp(min=1)
-        valid = pos_mask.any(dim=1)
-        return -mean_log_pos[valid].mean()
+
+        pos_counts = pos_mask.sum(dim=1, keepdim=True)
+        # アンカーごとの正例平均（SupConの 1/|P(i)| Σ_{p∈P(i)} log p を実装）
+        mean_log_pos = (pos_mask * log_prob).sum(dim=1, keepdim=True) / pos_counts.clamp(min=1.0)
+
+        valid = (pos_counts.squeeze(1) > 0)
+        if valid.any():
+            return -(mean_log_pos[valid]).mean()
+        # 極端にクラスが偏って正例ゼロ行しか無い場合のフォールバック
+        return torch.tensor(0.0, device=logits.device, dtype=logits.dtype)
 
     loss_t2a = _dir_loss(logits_t2a)
     if symmetric:
diff --git a/train_for_singleGPU.py b/train_for_singleGPU.py
index d5a335d..fbab8a9 100644
--- a/train_for_singleGPU.py
+++ b/train_for_singleGPU.py
@@ -1,11 +1,11 @@
 #!/usr/bin/env python3
-# train.py  ― supervised contrastive (source / space) + per-epoch retrieval eval
+# train_for_singleGPU.py  ― supervised contrastive (source / space) + per-epoch retrieval eval
 from utils.metrics import invariance_ratio
 import torch, torch.nn.functional as F
 from torch.utils.data import DataLoader
 from pathlib import Path
 import random, wandb, math, sys, yaml
-from utils.metrics import invariance_ratio, leakage_probe_acc, linear_probe_regression
+from utils.metrics import invariance_ratio
 
 from dataset.audio_rir_dataset import AudioRIRDataset, collate_fn   # 既存
 from dataset.precomputed_val_dataset import PrecomputedValDataset   # 追加①
@@ -13,7 +13,79 @@ from utils.metrics import cosine_sim, recall_at_k, recall_at_k_multi, eval_retri
 from model.delsa_model import DELSA                                # ← 修正
 
 random.seed(42)
+import torchaudio, torch
+from functools import lru_cache
+from dataset.audio_rir_dataset import foa_to_iv, FOA_SR, IV_SR  # 既存実装を再利用:contentReference[oaicite:15]{index=15}
+
+def _aformat_to_foa(wet):  # wet: [4,T] A-format -> FOA (W,Y,Z,X)
+    m0, m1, m2, m3 = wet[0], wet[1], wet[2], wet[3]
+    W = (m0+m1+m2+m3)/2; X = (m0+m1-m2-m3)/2
+    Y = (m0-m1+m2-m3)/2; Z = (m0-m1-m2+m3)/2
+    return torch.stack([W, Y, Z, X], dim=0)
+
+@lru_cache(maxsize=4096)
+def _load_rir_cpu_cached(path: str):
+    rir, sr = torchaudio.load(path)  # [4,Tr] A-format
+    return rir, sr
+
+def _fft_convolve_batch_gpu(dry_b, rir_b):  # [B,1,T],[B,4,Tr] -> [B,4,T]
+    if dry_b.size(1) == 1:
+        dry_b = dry_b.repeat(1,4,1)          # mono -> 4ch
+    n = dry_b.shape[-1] + rir_b.shape[-1] - 1
+    n_fft = 1 << (n - 1).bit_length()
+    D = torch.fft.rfft(dry_b, n_fft)         # CUDA対応のrFFT :contentReference[oaicite:16]{index=16}
+    R = torch.fft.rfft(rir_b, n_fft)
+    y = torch.fft.irfft(D * R, n_fft)[..., :n]
+    return y[..., :dry_b.shape[-1]]
+
+def _build_audio_from_defer(audio_list, device):
+    """Datasetから {dry, rir_path} を受け取り、GPUで畳み込み→FOA→16k→foa_to_iv までを実行。
+       戻り値は {"i_act","i_rea","omni_48k"} のテンソル（学習コードは無変更でOK）。"""
+    # 1) dry をまとめて GPU 転送
+    drys = torch.stack([a["dry"].squeeze(0) for a in audio_list]).unsqueeze(1).to(device, non_blocking=True)  # [B,1,T]
+
+    # 2) RIR をCPUキャッシュ→必要なら48kにresample→GPU転送
+    rirs_cpu = []
+    Tr_list = []
+    for a in audio_list:
+        rir, sr = _load_rir_cpu_cached(a["rir_path"])
+        rirs_cpu.append(rir if sr == FOA_SR else torchaudio.functional.resample(rir, sr, FOA_SR))
+        Tr_list.append(rir.shape[-1])
+
+        
+    Tr_max = max(Tr_list)                        # バッチ内の最大 RIR 長
+    pad_rirs = []
+    for rir in rirs_cpu:
+        pad = Tr_max - rir.shape[-1]
+        if pad > 0:
+            rir = torch.nn.functional.pad(rir, (0, pad))  # 末尾に 0 詰め（[4, Tr_max]）
+        pad_rirs.append(rir)
+
+
+    rirs = torch.stack(pad_rirs).to(device, non_blocking=True)  # [B,4,Tr]
+
+    # 3) GPUでFFT畳み込み（A-format）→ FOA 変換
+    wet_a = _fft_convolve_batch_gpu(drys, rirs)                 # [B,4,T]
+    foa   = torch.stack([_aformat_to_foa(w) for w in wet_a], dim=0)  # [B,4,T]
+    omni_48k = foa[:, 0, :]
+
+    # 4) 16kへリサンプル（環境により CPU 実装のため安全に .cpu() へ）
+    foa_16k = torchaudio.functional.resample(foa.detach().cpu(), orig_freq=FOA_SR, new_freq=IV_SR)
+
+    # 5) foa_to_iv（deviceに従ってCUDA/CPUで実行可。ここではCPUでも十分高速）:contentReference[oaicite:17]{index=17}
+    i_act_list, i_rea_list = [], []
+    for b in range(foa_16k.size(0)):
+        i_act, i_rea = foa_to_iv(foa_16k[b].unsqueeze(0))  # (1,3,F,Tfrm)
+        i_act_list.append(i_act.squeeze(0)); i_rea_list.append(i_rea.squeeze(0))
+    i_act = torch.stack(i_act_list, dim=0)
+    i_rea = torch.stack(i_rea_list, dim=0)
+
+    return {"i_act": i_act.to(device, non_blocking=True),
+            "i_rea": i_rea.to(device, non_blocking=True),
+            "omni_48k": omni_48k}  # omniは48kでそのままdevice上
 
+def _is_defer_format(audio_list):
+    return len(audio_list) > 0 and ("dry" in audio_list[0] and "rir_path" in audio_list[0])
 def _select_device(raw: str |None) -> str:
     if raw is None or raw.lower() == "auto":
         return "cuda" if torch.cuda.is_available() else "cpu"
@@ -52,24 +124,56 @@ def load_config(path: str | None = None) -> dict:
     cfg["device"] = _select_device(cfg["device"])
     return cfg
 
+
 def sup_contrast(a, b, labels, logit_scale, eps=1e-8, *, symmetric=True, exclude_diag=False):
+    """
+    Traditional Supervised Contrastive (Khosla et al., 2020) に準拠した2塔版。
+    - 同ラベルを正例。2塔なので a のアンカーに対して b 側の同ラベル全てが正例。
+    - exclude_diag=True のときは、対角 (i,i) を【分子・分母の両方】から外す（完全一貫）。
+      False のときは対角を分子・分母の両方に含める（同一インスタンス正例を含める）。
+    """
     a = F.normalize(a, dim=1); b = F.normalize(b, dim=1)
-    scale = torch.clamp(logit_scale, max=math.log(1e2)).exp()
+
+    # CLIP式のlogスケールを渡しているならこちらを使う：
+    # scale = torch.clamp(logit_scale, max=math.log(1e2)).exp()
+    # そうでなければ logit_scale は線形スカラーとして渡す
+    scale = logit_scale
+
     logits_t2a = (a @ b.T) * scale
     logits_a2t = logits_t2a.T if symmetric else None
 
+    labels = labels.view(-1)
+
     def _dir_loss(logits):
         B = logits.size(0)
-        pos_mask = labels[:, None].eq(labels[None, :])
-        max_sim, _ = logits.max(dim=1, keepdim=True)
-        logits = logits - max_sim.detach()
-        diag_mask = torch.eye(B, dtype=torch.bool, device=logits.device) if exclude_diag else torch.zeros(B,B,dtype=torch.bool, device=logits.device)
-        exp_sim = torch.exp(logits) * (~diag_mask)
-        denom = exp_sim.sum(dim=1, keepdim=True) + eps
+
+        # 正例マスク（同ラベル）。2塔なので「自分自身のベクトル」は存在しない。
+        pos_mask = labels[:, None].eq(labels[None, :]).float()
+
+        # 数値安定化
+        logits = logits - logits.max(dim=1, keepdim=True).values.detach()
+
+        # 分母マスク：exclude_diag によって対角を分子・分母で一貫処理
+        if exclude_diag:
+            pos_mask = pos_mask.clone()
+            pos_mask.fill_diagonal_(0.0)  # 分子からも対角を除外（完全一貫）
+            denom_mask = (~torch.eye(B, dtype=torch.bool, device=logits.device)).float()
+        else:
+            denom_mask = torch.ones_like(pos_mask)
+
+        exp_logits = torch.exp(logits) * denom_mask
+        denom = exp_logits.sum(dim=1, keepdim=True) + eps
         log_prob = logits - denom.log()
-        mean_log_pos = (log_prob * pos_mask.float()).sum(dim=1) / pos_mask.sum(dim=1).clamp(min=1)
-        valid = pos_mask.any(dim=1)
-        return -mean_log_pos[valid].mean()
+
+        pos_counts = pos_mask.sum(dim=1, keepdim=True)
+        # アンカーごとの正例平均（SupConの 1/|P(i)| Σ_{p∈P(i)} log p を実装）
+        mean_log_pos = (pos_mask * log_prob).sum(dim=1, keepdim=True) / pos_counts.clamp(min=1.0)
+
+        valid = (pos_counts.squeeze(1) > 0)
+        if valid.any():
+            return -(mean_log_pos[valid]).mean()
+        # 極端にクラスが偏って正例ゼロ行しか無い場合のフォールバック
+        return torch.tensor(0.0, device=logits.device, dtype=logits.dtype)
 
     loss_t2a = _dir_loss(logits_t2a)
     if symmetric:
@@ -147,8 +251,14 @@ def main():
         model.train()
         for step, batch in enumerate(train_dl, 1):
             if batch is None: continue
+            audio_list = batch["audio"]
+
+            if _is_defer_format(audio_list):
+                audio = _build_audio_from_defer(audio_list, cfg["device"]) 
+            else:
+                audio = {k: torch.stack([d[k] for d in batch["audio"]]).to(cfg["device"]) for k in ("i_act","i_rea","omni_48k")}
+
             batch_data = {k: recursive_to(v, cfg["device"]) for k, v in batch.items() if k not in ["audio","texts"]}
-            audio = {k: torch.stack([d[k] for d in batch["audio"]]).to(cfg["device"]) for k in ("i_act","i_rea","omni_48k")}
             texts  = batch["texts"]
             src_lb = batch["source_id"].reshape(-1).to(cfg["device"])
             spa_lb = batch["space_id"].reshape(-1).to(cfg["device"])
diff --git a/utils/__pycache__/metrics.cpython-312.pyc b/utils/__pycache__/metrics.cpython-312.pyc
index 62f62c5..f023462 100644
Binary files a/utils/__pycache__/metrics.cpython-312.pyc and b/utils/__pycache__/metrics.cpython-312.pyc differ
diff --git a/wandb/debug-internal.log b/wandb/debug-internal.log
index 03c8713..1c372b4 120000
--- a/wandb/debug-internal.log
+++ b/wandb/debug-internal.log
@@ -1 +1 @@
-run-20250812_173509-3vs81wec/logs/debug-internal.log
\ No newline at end of file
+run-20250816_233322-pjjzouzt/logs/debug-internal.log
\ No newline at end of file
diff --git a/wandb/debug.log b/wandb/debug.log
index c336e74..9457d37 120000
--- a/wandb/debug.log
+++ b/wandb/debug.log
@@ -1 +1 @@
-run-20250812_173509-3vs81wec/logs/debug.log
\ No newline at end of file
+run-20250816_233322-pjjzouzt/logs/debug.log
\ No newline at end of file
diff --git a/wandb/latest-run b/wandb/latest-run
index 04de0af..b4744a4 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20250812_173509-3vs81wec
\ No newline at end of file
+run-20250816_233322-pjjzouzt
\ No newline at end of file
diff --git a/wandb/run-20250812_165834-qjzxr8ux/files/code/train.py b/wandb/run-20250812_165834-qjzxr8ux/files/code/train.py
index fee1b70..4a4f3b9 100644
--- a/wandb/run-20250812_165834-qjzxr8ux/files/code/train.py
+++ b/wandb/run-20250812_165834-qjzxr8ux/files/code/train.py
@@ -7,7 +7,7 @@ from pathlib import Path
 import random, wandb, math, sys, yaml
 from utils.metrics import invariance_ratio, leakage_probe_acc, linear_probe_regression
 
-from dataset.audio_rir_dataset import AudioRIRDataset, collate_fn   # 既存
+from dataset.audio_rir_dataset_old import AudioRIRDataset, collate_fn   # 既存
 from dataset.precomputed_val_dataset import PrecomputedValDataset   # 追加①
 from utils.metrics import cosine_sim, recall_at_k, recall_at_k_multi                   # 追加②
 from model.delsa_model import DELSA                                # ← 修正
diff --git a/wandb/run-20250812_165915-0vikrk70/files/code/train.py b/wandb/run-20250812_165915-0vikrk70/files/code/train.py
index fee1b70..4a4f3b9 100644
--- a/wandb/run-20250812_165915-0vikrk70/files/code/train.py
+++ b/wandb/run-20250812_165915-0vikrk70/files/code/train.py
@@ -7,7 +7,7 @@ from pathlib import Path
 import random, wandb, math, sys, yaml
 from utils.metrics import invariance_ratio, leakage_probe_acc, linear_probe_regression
 
-from dataset.audio_rir_dataset import AudioRIRDataset, collate_fn   # 既存
+from dataset.audio_rir_dataset_old import AudioRIRDataset, collate_fn   # 既存
 from dataset.precomputed_val_dataset import PrecomputedValDataset   # 追加①
 from utils.metrics import cosine_sim, recall_at_k, recall_at_k_multi                   # 追加②
 from model.delsa_model import DELSA                                # ← 修正
diff --git a/wandb/run-20250812_173509-3vs81wec/files/code/train_for_singleGPU.py b/wandb/run-20250812_173509-3vs81wec/files/code/train_for_singleGPU.py
index 65a78af..14a0744 100644
--- a/wandb/run-20250812_173509-3vs81wec/files/code/train_for_singleGPU.py
+++ b/wandb/run-20250812_173509-3vs81wec/files/code/train_for_singleGPU.py
@@ -7,7 +7,7 @@ from pathlib import Path
 import random, wandb, math, sys, yaml
 from utils.metrics import invariance_ratio, leakage_probe_acc, linear_probe_regression
 
-from dataset.audio_rir_dataset import AudioRIRDataset, collate_fn   # 既存
+from dataset.audio_rir_dataset_old import AudioRIRDataset, collate_fn   # 既存
 from dataset.precomputed_val_dataset import PrecomputedValDataset   # 追加①
 from utils.metrics import cosine_sim, recall_at_k, recall_at_k_multi                   # 追加②
 from model.delsa_model import DELSA                                # ← 修正
