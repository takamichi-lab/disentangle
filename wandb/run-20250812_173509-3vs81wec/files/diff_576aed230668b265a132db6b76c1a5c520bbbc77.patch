diff --git a/config.yaml b/config.yaml
index d091a8e..866c9ac 100644
--- a/config.yaml
+++ b/config.yaml
@@ -4,19 +4,24 @@ audio_csv_train: AudioCaps_csv/train.csv
 rir_csv_train:   RIR_dataset/rir_catalog_train.csv
 
 # val
-audio_csv_val: AudioCaps_csv/val.csv        # 省略可
-rir_csv_val: RIR_dataset/rir_catalog_val.csv        # 省略可
+audio_csv_val: val_fixed_400x24/audio_fixed.csv      # 省略可
+rir_csv_val: val_fixed_400x24/rir_fixed.csv        # 省略可
 out_dir: /home/takamichi-lab-pc09/DELSA/Spatial_AudioCaps/takamichi09/for_delsa_spatialAudio 
 #予め作っておく クラスタ分散とMIG用
 SpatialAudio_csv_val: /home/takamichi-lab-pc09/DELSA/Spatial_AudioCaps/takamichi09/for_delsa_spatialAudio 
 
+# 前計算Valの場所（precompute_val.py の out_dir）
+val_precomp_root: Spatial_AudioCaps/takamichi09/for_delsa_spatialAudio
+val_index_csv:    Spatial_AudioCaps/takamichi09/for_delsa_spatialAudio/val_precomputed.csv   # 省略可（rootからの既定を使う）
+val_batch_size: 16
+
 audio_base: AudioCaps_mp3 
 batch_size: 8
-n_views: 2
+n_views: 5
 val_n_views: 24
 epochs: 5
 lr: 0.0001
 device: auto            # "auto" → cuda if available
 wandb: true
 proj: delsa-sup-contrast
-run_name: None
\ No newline at end of file
+run_name: 
\ No newline at end of file
diff --git a/dataset/__pycache__/audio_rir_dataset.cpython-312.pyc b/dataset/__pycache__/audio_rir_dataset.cpython-312.pyc
index c177722..363972c 100644
Binary files a/dataset/__pycache__/audio_rir_dataset.cpython-312.pyc and b/dataset/__pycache__/audio_rir_dataset.cpython-312.pyc differ
diff --git a/dataset/audio_rir_dataset.py b/dataset/audio_rir_dataset.py
index d6793d5..db3ae5d 100644
--- a/dataset/audio_rir_dataset.py
+++ b/dataset/audio_rir_dataset.py
@@ -7,6 +7,11 @@ from torch.nn.utils.rnn import pad_sequence
 from collections import defaultdict
 import numpy as np
 import soundfile   as sf
+try:
+    if torchaudio.get_audio_backend() != "sox_io":
+        torchaudio.set_audio_backend("sox_io")
+except Exception:
+    pass
 # ToDo;SALSAの論文の実装に合わせる。0のときの処理を
  #ToDo: A-format to B-format
  # #ToDo: captionも空間拡張する
@@ -92,7 +97,8 @@ class AudioRIRDataset(Dataset):
                  share_rir: bool = True,
                  batch_size : int | None = None,
                  stats_path: str = "/home/takamichi-lab-pc09/DELSA/RIR_dataset/stats.pt",
-                 hop: int =100):
+                 hop: int =100,
+                 bad_log_path: str | None = "bad_mp3_ids.txt"):
         super().__init__()
         # ── 設定ロード ──
 
@@ -121,7 +127,8 @@ class AudioRIRDataset(Dataset):
 
         # RIR ごとのメタ dict
         self.rir_meta  = {row["rir_path"]: row.to_dict() for _, row in rir_df.iterrows()}
-        
+        self._bad_ids = set()
+        self._bad_log_path = Path(bad_log_path) if bad_log_path else None    
         #物理量を正規化するためののコード
         stats = torch.load(stats_path)
         self.area_mean = stats["area_m2"]["mean"]
@@ -134,7 +141,16 @@ class AudioRIRDataset(Dataset):
 
     def __len__(self):
         return len(self.audio_df)
-    
+    def _mark_bad(self, audiocap_id: int, err: Exception):
+        if audiocap_id in self._bad_ids:
+            return
+        self._bad_ids.add(audiocap_id)
+        msg = f"[WARN] skip {audiocap_id}: {err}"
+        print(msg)
+        if self._bad_log_path:
+            with self._bad_log_path.open("a") as f:
+                f.write(f"{audiocap_id}\t{type(err).__name__}\t{err}\n")
+ 
     def _load_dry(self, audiocap_id: int) -> torch.Tensor:
         path = self.base_dir / self.split / f"{audiocap_id}.mp3"
         wav, sr = torchaudio.load(path)
@@ -161,7 +177,7 @@ class AudioRIRDataset(Dataset):
         Z =  (m0 -  m1 -  m2 +  m3)/2
         foa = torch.stack([W, Y, Z, X])
         #print(sr)
-        sf.write('foa.wav', foa.T, sr)
+        #sf.write('foa.wav', foa.T, sr)
         return foa
 
 
@@ -169,8 +185,11 @@ class AudioRIRDataset(Dataset):
         row = self.audio_df.iloc[idx]
         audiocap_id = row["audiocap_id"]
         caption = row["caption"]
-
-        dry = self._load_dry(audiocap_id)
+        try:
+            dry = self._load_dry(audiocap_id)
+        except Exception as e:
+            self._mark_bad(audiocap_id, e)
+            return None  # ← ここがポイント：丸ごとスキップ
 
         audio_features_list = []
         src_ids, spa_ids, texts, rir_meta = [], [], [], []
@@ -230,6 +249,11 @@ class AudioRIRDataset(Dataset):
     
 # ---------------- collate_fn (4 ch → 特徴辞書) -----------------------------
 def collate_fn(batch):
+    # 失敗サンプル（None）を取り除く
+    batch = [b for b in batch if b is not None]
+    if len(batch) == 0:
+        return None
+    # 以降は従来どおり flatten
     # 既存の flatten 処理
     audio_list, text_list, src_list, spa_list, rir_meta_list = [], [], [], [], []
     for sample in batch:
diff --git a/foa.wav b/foa.wav
index 1df7961..d97190c 100644
Binary files a/foa.wav and b/foa.wav differ
diff --git a/model/__pycache__/delsa_model.cpython-312.pyc b/model/__pycache__/delsa_model.cpython-312.pyc
index 9c9ddf6..a2c4f5b 100644
Binary files a/model/__pycache__/delsa_model.cpython-312.pyc and b/model/__pycache__/delsa_model.cpython-312.pyc differ
diff --git a/model/delsa_model.py b/model/delsa_model.py
index 7154414..e8bde49 100644
--- a/model/delsa_model.py
+++ b/model/delsa_model.py
@@ -1,4 +1,4 @@
-# file: models/delsa_model.py
+# file: model/delsa_model.py
 
 import torch
 import torch.nn as nn
diff --git a/precompute_val.py b/precompute_val.py
index c1c86ba..bb5f5be 100644
--- a/precompute_val.py
+++ b/precompute_val.py
@@ -1,44 +1,50 @@
+#!/usr/bin/env python3
 # scripts/precompute_val.py
-import argparse, json, torchaudio, torch, pandas as pd
-from pathlib import Path
-from dataset.audio_rir_dataset import AudioRIRDataset, foa_to_iv, rewrite_caption
-import torch 
-import torch.nn.functional as F
-from torch.utils.data import DataLoader
+# Dry×RIR を事前畳み込みして FOA/IV を保存し、val_precomputed.csv を作成
+# 進捗は tqdm の単一バーで表示（総ビュー数 = Audio件数 × n_views）
+
+import argparse
 from pathlib import Path
-import random
-import wandb
-import math
 import sys
+import random
 import yaml
-random.seed(42); torch.manual_seed(42)
-def _select_device(raw: str |None) -> str:
+import torch
+import torchaudio
+import pandas as pd
+from tqdm.auto import tqdm
+
+from dataset.audio_rir_dataset import AudioRIRDataset, foa_to_iv, rewrite_caption
+
+# 再現性
+random.seed(42)
+torch.manual_seed(42)
+
+def _select_device(raw: str | None) -> str:
     if raw is None or raw.lower() == "auto":
         return "cuda" if torch.cuda.is_available() else "cpu"
     return raw
 
 def load_config(path: str | None = None) -> dict:
-    """Load hyper‑parameters from a YAML file and fill in sane defaults."""
-
-    # 1️⃣  determine YAML path (CLI arg 1 or default "config.yaml")
+    """Load hyper-parameters from a YAML file and fill in sane defaults."""
+    # 1) YAML パス決定（CLIの第1引数が .yml/.yaml ならそれを優先）
     if path is None:
         if len(sys.argv) > 1 and sys.argv[1].endswith((".yml", ".yaml")):
             path = sys.argv[1]
         else:
             path = "config.yaml"
 
-    # 2️⃣  read YAML
+    # 2) 読み込み
     try:
-        with open(path, "r", encoding="utf‑8") as f:
+        with open(path, "r", encoding="utf-8") as f:
             cfg = yaml.safe_load(f) or {}
-    except FileNotFoundError as e:
-        raise SystemExit(f"[ERR] YAML not found: {path}") from e
+    except FileNotFoundError:
+        cfg = {}
 
-    # 3️⃣  defaults
+    # 3) デフォルト値
     defaults = {
         "split": "train",
         "batch_size": 8,
-        "n_views": 4,
+        "n_views": 24,  # 前計算で使う RIR 本数（<=0 なら全部）
         "epochs": 5,
         "lr": 0.0001,
         "device": "auto",
@@ -51,72 +57,124 @@ def load_config(path: str | None = None) -> dict:
         "rir_csv_val": "RIR_dataset/rir_catalog_val.csv",
         "base_dir": "RIR_dataset",
         "audio_base": None,
-
+        "out_dir": "data/val_precomputed",
     }
     for k, v in defaults.items():
         cfg.setdefault(k, v)
 
-    # 4️⃣  post‑process
+    # 4) 後処理
     cfg["device"] = _select_device(cfg["device"])
     return cfg
 
-def main(args):
+def main(args: argparse.Namespace):
     cfg = load_config()
-    if args.csv_audio: cfg["audio_csv_val"] = args.csv_audio
-    if args.csv_rir: cfg["rir_csv_val"] = args.csv_rir
-    if args.base_dir: cfg["audio_base"] = args.base_dir
-
+    # CLI で上書き
+    if args.csv_audio:
+        cfg["audio_csv_val"] = args.csv_audio
+    if args.csv_rir:
+        cfg["rir_csv_val"] = args.csv_rir
+    if args.base_dir:
+        cfg["audio_base"] = args.base_dir
     cfg["out_dir"] = args.out_dir or cfg.get("out_dir", "data/val_precomputed")
-    if args.n_views is not None:
-        cfg["n_views_val"] = args.n_views
 
+    # n_views（<=0 なら「全部」）
+    n_views_cfg = (
+        args.n_views
+        if args.n_views is not None
+        else cfg.get("val_n_views", cfg.get("n_views", 1))
+    )
+    # Dataset 準備（学習用の挙動は使わないが整合のため値は渡す）
     ds = AudioRIRDataset(
-        csv_audio=cfg["audio_csv_val"], base_dir=cfg["audio_base"],
-        csv_rir=cfg["rir_csv_val"],     split="val",
-        n_views=cfg["n_views_val"],     share_rir=False,  # ←ランダム性を排除
-        batch_size=None)
-    out_root = Path(cfg["out_dir"]); out_root.mkdir(parents=True, exist_ok=True)
-    records = []
-
+        csv_audio=cfg["audio_csv_val"],
+        base_dir=cfg["audio_base"],
+        csv_rir=cfg["rir_csv_val"],
+        split="val",
+        n_views=max(1, n_views_cfg if isinstance(n_views_cfg, int) else 1),
+        share_rir=False,
+        batch_size=None,
+    )
+
+    out_root = Path(cfg["out_dir"])
+    (out_root / "foa").mkdir(parents=True, exist_ok=True)
+    (out_root / "feat").mkdir(parents=True, exist_ok=True)
+
+    records: list[dict] = []
+
+    # RIR の決定論的順序
     rir_list = sorted(ds.rir_paths)
-    # n_views <=0を「全部のRIRを使う」と解釈
-    n_views = len(rir_list) if cfg["n_views"] <= 0 else min(cfg["n_views"],len(rir_list))
+    n_views = len(rir_list) if (isinstance(n_views_cfg, int) and n_views_cfg <= 0) else min(int(n_views_cfg), len(rir_list))
 
     torch.set_grad_enabled(False)
-    for idx in range(len(ds)):
-
-        row = ds.audio_df.iloc[idx]
-        dry   = ds._load_dry(row["audiocap_id"])
-        for rir_path in rir_list[:n_views]:
-            foa   = ds._apply_rir(dry, rir_path)            # 4-ch FOA:contentReference[oaicite:0]{index=0}
-            foa16 = torchaudio.functional.resample(foa, 48_000, 16_000)
-            # 1) タプル全体を受け取る
-            i_act, i_rea = foa_to_iv(foa16.unsqueeze(0))  # ← [0] を削除
-
-            # 2) バッチ次元 (B=1) を潰す
-            i_act, i_rea = i_act.squeeze(0), i_rea.squeeze(0)
-            # --- 保存 ---
-            key   = f"{row.audiocap_id}_{Path(rir_path).stem}"
-            (out_root / "foa").mkdir(exist_ok=True)
-            (out_root / "feat").mkdir(exist_ok=True)
-            torchaudio.save(out_root/f"foa/{key}.wav", foa, 48_000)
-            torch.save({'i_act': i_act, 'i_rea': i_rea}, out_root/f"feat/{key}.pt")
-
-            meta   = ds.rir_meta[rir_path].copy()
-            caption= rewrite_caption(row["caption"], meta)
-            records.append({
-                "audiocap_id": row.audiocap_id,
-                "rir_path": rir_path,
-                "foa_path": f"foa/{key}.wav",
-                "feat_path": f"feat/{key}.pt",
-                "caption": caption
-            })
-
-    pd.DataFrame(records).to_csv(out_root/"val_precomputed.csv", index=False)
+    total_views = len(ds) * n_views
+    # tqdm開始前に追加
+    print(f"[INFO] Audio clips={len(ds)} | RIR candidates={len(rir_list)} | "
+      f"n_views(effective)={n_views} | total views={len(ds)*n_views}")
+    pbar = tqdm(total=total_views, desc="Precomputing FOA/IV", unit="view")
+
+    with torch.no_grad():
+        for idx in range(len(ds)):
+            row = ds.audio_df.iloc[idx]
+            dry = ds._load_dry(row["audiocap_id"])  # (T,)
+            for j, rir_path in enumerate(rir_list[:n_views]):
+                # 4ch FOA を合成（48k）
+                foa = ds._apply_rir(dry, rir_path)  # (4, T_48k)
+
+                # FOA 48k を保存（16-bit PCM）
+                key = f"{row.audiocap_id}_{Path(rir_path).stem}"
+                torchaudio.save(
+                    out_root / f"foa/{key}.wav",
+                    foa,
+                    48_000,
+                    format="wav",
+                    encoding="PCM_S",
+                    bits_per_sample=16,
+                )
+
+                # FOA→16k へリサンプルして IV 特徴を作成
+                foa16 = torchaudio.functional.resample(foa, 48_000, 16_000)  # (4, T_16k)
+                i_act, i_rea = foa_to_iv(foa16.unsqueeze(0))  # (1,3,F,T) ×2
+                i_act, i_rea = i_act.squeeze(0), i_rea.squeeze(0)  # (3,F,T)
+
+                # 特徴保存
+                torch.save({"i_act": i_act, "i_rea": i_rea}, out_root / f"feat/{key}.pt")
+
+                # メタ→キャプション
+                meta = ds.rir_meta[rir_path].copy()
+                caption = rewrite_caption(row["caption"], meta)
+
+                records.append(
+                    {
+                        "audiocap_id": row.audiocap_id,
+                        "rir_path": rir_path,
+                        "foa_path": f"foa/{key}.wav",
+                        "feat_path": f"feat/{key}.pt",
+                        "caption": caption,
+                    }
+                )
+
+                # 進捗
+                if (j % 4) == 0:
+                    pbar.set_postfix({"clip": int(row["audiocap_id"]), "RIR": Path(rir_path).stem})
+                pbar.update(1)
+
+    pbar.close()
+
+    # 索引CSV
+    df = pd.DataFrame(records)
+    df.to_csv(out_root / "val_precomputed.csv", index=False)
+
+    # 簡易サマリ
+    print(f"[OK] Wrote {len(df)} views to {out_root.resolve()}")
+    print(f" - Audio clips: {len(ds)}")
+    print(f" - RIR used per clip: {n_views}")
+    print(f" - Output CSV: {out_root / 'val_precomputed.csv'}")
 
 if __name__ == "__main__":
     p = argparse.ArgumentParser()
-    p.add_argument("--csv_audio"); p.add_argument("--base_dir")
-    p.add_argument("--csv_rir");  p.add_argument("--out_dir")
-    p.add_argument("--n_views", type=int, help="<=0 なら RIR 全部を使用")
+    p.add_argument("--config", help="YAML path (default: config.yaml)")
+    p.add_argument("--csv_audio", help="Validation audio CSV (e.g., AudioCaps_csv/val.csv)")
+    p.add_argument("--base_dir", help="Base directory for audio files")
+    p.add_argument("--csv_rir", help="Validation RIR catalog CSV (must include 'rir_path')")
+    p.add_argument("--out_dir", help="Output root (default: data/val_precomputed)")
+    p.add_argument("--n_views", type=int, help="<=0 なら RIR 全部を使用（デフォルトは config.yaml の n_views）")
     main(p.parse_args())
diff --git a/train.py b/train.py
deleted file mode 100644
index db9318a..0000000
--- a/train.py
+++ /dev/null
@@ -1,391 +0,0 @@
-#!/usr/bin/env python3
-# train_sup.py  ― supervised contrastive (source / space) 版
-import torch 
-import torch.nn.functional as F
-from torch.utils.data import DataLoader
-from pathlib import Path
-import random
-import wandb
-import math
-import sys
-import yaml
-
-
-
-
-# 容積の分類のloss
-# 音源の分類のloss 犬とか
-# adversarial loss 
-    # sourceの埋め込みからはspaceの埋め込みを予測できないようにする。
-    # 音源の分類は　spatialの埋め込みからはできないようにする。
-
-# 3週間くらいで結果揃える
-# ToDo: 正規化をする. 
-
-
-random.seed(42)  # 再現性のため
-
-
-#ToDO:Supervised Contrasive Learningを読む。https://proceedings.neurips.cc/paper_files/paper/2020/file/d89a66c7c80a29b1bdbab0f2a1a94af8-Paper.pdf?utm_source=chatgpt.com
-# ────────────────────────── CLI ──────────────────────────
-
-def _select_device(raw: str |None) -> str:
-    if raw is None or raw.lower() == "auto":
-        return "cuda" if torch.cuda.is_available() else "cpu"
-    return raw
-
-def load_config(path: str | None = None) -> dict:
-    """Load hyper‑parameters from a YAML file and fill in sane defaults."""
-
-    # 1️⃣  determine YAML path (CLI arg 1 or default "config.yaml")
-    if path is None:
-        if len(sys.argv) > 1 and sys.argv[1].endswith((".yml", ".yaml")):
-            path = sys.argv[1]
-        else:
-            path = "config.yaml"
-
-    # 2️⃣  read YAML
-    try:
-        with open(path, "r", encoding="utf‑8") as f:
-            cfg = yaml.safe_load(f) or {}
-    except FileNotFoundError as e:
-        raise SystemExit(f"[ERR] YAML not found: {path}") from e
-
-    # 3️⃣  defaults
-    defaults = {
-        "split": "train",
-        "batch_size": 8,
-        "n_views": 4,
-        "epochs": 5,
-        "lr": 0.0001,
-        "device": "auto",
-        "wandb": True,
-        "proj": "delsa-sup-contrast",
-        "run_name": None,
-    }
-    for k, v in defaults.items():
-        cfg.setdefault(k, v)
-
-    # 4️⃣  post‑process
-    cfg["device"] = _select_device(cfg["device"])
-    return cfg
-
-
-
-
-
-# ───────── Supervised cross-modal contrastive los ─────────
-
-def sup_contrast(
-    a: torch.Tensor,
-    b: torch.Tensor,
-    labels: torch.Tensor,
-    logit_scale: torch.Tensor,
-    eps: float = 1e-8,
-    *,
-    symmetric: bool = True,      # True: Text↔Audio 両方向を平均
-    exclude_diag: bool = False   # True: 分母から自己相似を除外 (SCL 型)
-) -> torch.Tensor:
-    """
-    Cross‑modal Supervised Contrastive Loss (InfoNCE‑style).
-
-    Parameters
-    ----------
-    a, b        : [B, D]  L2‑normalized以外は何でも可。内部で normalize する。
-    labels      : [B]     同じ値を持つサンプルを正例集合とみなす。
-    logit_scale : ()      log(temperature^{-1})。CLIP と同じ扱い。
-    eps         : float   分母のゼロ除避け。
-    symmetric   : bool    True  -> (Text→Audio + Audio→Text)/2
-    exclude_diag: bool    True  -> 自己相似を分母から除外 (SCL 推奨)
-
-    Returns
-    -------
-    loss : torch.Tensor  scalar
-    """
-
-    # 1. L2‑normalize
-    a = F.normalize(a, dim=1)
-    b = F.normalize(b, dim=1)
-
-    # 2. temperature (clamp to avoid overflow)
-    scale = torch.clamp(logit_scale, max=math.log(1e2)).exp()
-
-    # 3. logits  (Text→Audio)
-    logits_t2a = torch.matmul(a, b.T) * scale           # [B, B]
-
-    # optional: Audio→Text (reuse transpose for efficiency)
-    logits_a2t = logits_t2a.T if symmetric else None
-
-    def _directional_loss(logits: torch.Tensor) -> torch.Tensor:
-        B = logits.size(0)
-
-        # --- positive mask ---
-        pos_mask = labels[:, None].eq(labels[None, :])          # [B, B] bool
-
-        # --- numerical stability (detach!) ---
-        max_sim, _ = logits.max(dim=1, keepdim=True)
-        logits_stable = logits - max_sim.detach()               # stop‑grad on max
-
-        # --- denominator ---
-        if exclude_diag:
-            diag_mask = torch.eye(B, dtype=torch.bool, device=logits.device)
-        else:
-            diag_mask = torch.zeros(B, B, dtype=torch.bool, device=logits.device)
-
-        exp_sim = torch.exp(logits_stable) * (~diag_mask)
-        denom = exp_sim.sum(dim=1, keepdim=True) + eps
-
-        # --- log‑probabilities ---
-        log_prob = logits_stable - denom.log()
-        mean_log_pos = (log_prob * pos_mask.float()).sum(dim=1) / pos_mask.sum(dim=1).clamp(min=1)
-
-        # --- drop rows with no positives ---
-        valid = pos_mask.any(dim=1)
-        return -mean_log_pos[valid].mean()
-
-    loss_t2a = _directional_loss(logits_t2a)
-    if symmetric:
-        loss_a2t = _directional_loss(logits_a2t)
-        return 0.5 * (loss_t2a + loss_a2t)
-    else:
-        return loss_t2a
-# ToDo;sup_contrastの公式実装見る：https://github.com/HobbitLong/SupContrast?utm_source=chatgpt.com
-
-
-# ── 物理量の予測損失 ──
-def physical_loss(model_output, batch_data,isNorm=True ,dataloader=None) -> torch.Tensor:
-    """
-    物理量の予測損失を計算する。
-    pred_physics: {
-        "direction": [B, 2]  # azimuth, elevation
-        "area": [B, 1]
-        "distance": [B, 1]
-        "reverb": [B, 1] """
-    if isNorm:
-        pred_physics = {
-            "direction": model_output["direction"],  
-            "area": model_output["area"],
-            "distance": model_output["distance"],
-            "reverb": model_output["reverb"]}  
-   
-        true_physics = {
-            "direction": batch_data["rir_meta"]["direction_vec"],  
-            "area": batch_data["rir_meta"]["area_m2_norm"],
-            "distance": batch_data["rir_meta"]["distance_norm"],
-            "reverb": batch_data["rir_meta"]["t30_norm"]} 
-        
-    else: #生の値で評価するとき
-        area_mean, area_std = dataloader.dataset.area_mean, dataloader.dataset.area_std
-        distance_mean, distance_std = dataloader.dataset.dist_mean, dataloader.dataset.dist_std
-        t30_mean, t30_std = dataloader.dataset.t30_mean, dataloader.dataset.t30_std
-        pred_physics = {
-            "direction": model_output["direction"],  
-            "area": model_output["area"] * area_std + area_mean,
-            "distance": model_output["distance"] * distance_std + distance_mean,
-            "reverb": model_output["reverb"] * t30_std + t30_mean
-        }
-        true_physics = {
-            "direction": batch_data["rir_meta"]["direction_vec"],
-            "area": batch_data["rir_meta"]["area_m2"],
-            "distance": batch_data["rir_meta"]["distance"],
-            "reverb": batch_data["rir_meta"]["fullband_T30_ms"]}    
-        
-    # 方向の損失は、cosine similarityを使う。
-    loss_dir = (-1* F.cosine_similarity(pred_physics["direction"], true_physics["direction"], dim=1)).mean()
-
-    #mse 自体がえ平均をとるので、平均をとる必要はない。
-    loss_area = F.mse_loss(pred_physics["area"].squeeze(-1), true_physics["area"])
-    loss_distance= F.mse_loss(pred_physics["distance"].squeeze(-1), true_physics["distance"])
-    loss_reverb = F.mse_loss(pred_physics["reverb"].squeeze(-1), true_physics["reverb"])
-
-    total_physical_loss = loss_dir + loss_area + loss_distance + loss_reverb
-    physical_losses = {
-        "total_loss": total_physical_loss, "loss_dir": loss_dir.item(),
-        "loss_distance": loss_distance.item(), "loss_area": loss_area.item(), "loss_reverb": loss_reverb.item()
-
-    }
-    return physical_losses, total_physical_loss
-    
-def recursive_to(obj, device):
-    """Tensor だけを再帰的に device へ送る"""
-    if isinstance(obj, torch.Tensor):
-        return obj.to(device)
-    if isinstance(obj, dict):
-        return {k: recursive_to(v, device) for k, v in obj.items()}
-    if isinstance(obj, list):
-        return [recursive_to(v, device) for v in obj]
-    return obj          # int / float / str など
-# ─────────────────────────── Main ─────────────────────────
-def main():
-    cfg = load_config()  # config.yaml を読み込む
-    from dataset.audio_rir_dataset import AudioRIRDataset, collate_fn
-    if cfg["wandb"]:
-        wandb.init(project=cfg["proj"],
-                   name = cfg["run_name"],
-                   config = cfg, save_code =True,
-                   mode="online")
-    train_ds = AudioRIRDataset(
-        csv_audio=cfg["audio_csv_train"],
-        base_dir=cfg["audio_base"],
-        csv_rir=cfg["rir_csv_train"],
-        n_views=cfg["n_views"],
-        split=cfg["split"],
-        batch_size=cfg["batch_size"]
-    )
-    train_dl = DataLoader(train_ds, batch_size=cfg["batch_size"],
-                    shuffle=True, num_workers=4,
-                    collate_fn=collate_fn, pin_memory=False)
-    
-    # val 用  ★追加★
-    val_ds = AudioRIRDataset(
-        csv_audio=cfg["audio_csv_val"], base_dir=cfg["audio_base"],
-        csv_rir=cfg["rir_csv_val"], split="val",             # ここだけ split=val
-        n_views=cfg["n_views"],
-        batch_size=cfg["batch_size"])
-    val_dl = DataLoader(val_ds,  batch_size=cfg["batch_size"],
-                        shuffle=False, num_workers=4,
-                        collate_fn=collate_fn, pin_memory=False)
-    from model.delsa_model import DELSA      # アップロード済みモデル
-    model = DELSA(audio_encoder_cfg={}, text_encoder_cfg={}).to(cfg["device"])
-    opt   = torch.optim.AdamW(model.parameters(), lr=cfg["lr"])
-
-    for ep in range(1, cfg["epochs"]+1):
-        model.train()
-        for step, batch in enumerate(train_dl, 1):
-            if batch is None:
-                continue
-            # ── データをデバイスに転送 ──
-            #batch_data = {k: v.to(cfg["device"]) for k, v in batch.items() if k not in ["audio", "texts"]}
-            batch_data = {k: recursive_to(v, cfg["device"])
-              for k, v in batch.items() if k not in ["audio","texts"]}
-            # ----------- flatten ------------
-
-            audio_dict = {k: torch.stack([d[k] for d in batch["audio"]]).to(cfg["device"])
-              for k in ("i_act","i_rea","omni_48k")}
-            texts  = batch["texts"]
-            #print(texts)
-            src_lb = batch["source_id"].reshape(-1).to(cfg["device"])  # [B']
-            meta = batch["rir_meta"]  # RIRメタデータ
-            #print(meta)
-            spa_lb = batch["space_id"].reshape(-1).to(cfg["device"])
-            #print(spa_lb)
-            # ----------- forward ------------
-            model_out = model(audio_dict, texts)   # expect 4 embeddings + logit_scale
-
-            a_s  = F.normalize(model_out["audio_space_emb"],  dim=-1)
-            t_s  = F.normalize(model_out["text_space_emb"],   dim=-1)
-            a_src= F.normalize(model_out["audio_source_emb"], dim=-1)
-            t_src= F.normalize(model_out["text_source_emb"],  dim=-1)
-
-
-
-            T = model_out["logit_scale"].exp()     # 温度
-            loss_space  = sup_contrast(a_s,  t_s,  spa_lb, T)
-            loss_source = sup_contrast(a_src,t_src,src_lb, T)
-
-            physical_losses, total_physical_loss = physical_loss(model_out, batch_data, isNorm=True, dataloader=train_dl)
-
-            loss = (loss_space + loss_source + total_physical_loss)
-
-            opt.zero_grad()
-            loss.backward()
-            opt.step()
-
-            if step % 10 == 0:
-                print(f"Epoch {ep} Step {step}/{len(train_dl)}  "
-                      f"space={loss_space:.4f}  src={loss_source:.4f}")
-                if cfg["wandb"]:
-                    wandb.log({
-                        "loss/space": loss_space.item(),
-                        "loss/source": loss_source.item(),
-                        "loss/physical": total_physical_loss.item(),
-                        "loss/dir": physical_losses["loss_dir"],
-                        "loss/distance": physical_losses["loss_distance"],
-                        "loss/area": physical_losses["loss_area"],  
-                        "loss/reverb": physical_losses["loss_reverb"],
-                        "logit_scale": model_out["logit_scale"].item(),
-                        "loss/mean": loss.item(),
-                        "epoch": ep,
-                        "step": step + (ep-1)*len(train_dl)
-                    })
-        # ---------- Validation ----------  ★追加ブロック★
-        model.eval()
-        val_metrics = {"space": 0.0, "source": 0.0,
-                    "physical": 0.0, "count": 0}
-        with torch.no_grad():
-            for batch in val_dl:
-                if batch is None:  # collate_fn が None を返すケース対策
-                    continue
-                # --- デバイス転送／forward は train ループと同じ ---
-                batch_data = {k: v.to(cfg["device"]) for k, v in batch.items()
-                            if k not in ["audio","texts"]}
-                audio_dict = {k: torch.stack([d[k] for d in batch["audio"]]).to(args.device)
-                            for k in ("i_act","i_rea","omni_48k")}
-                texts  = batch["texts"]
-                src_lb = batch["source_id"].reshape(-1).to(cfg["device"])
-                spa_lb = batch["space_id"].reshape(-1).to(cfg["device"])
-
-                model_out = model(audio_dict, texts)
-
-                a_s  = F.normalize(model_out["audio_space_emb"],  dim=-1)
-                t_s  = F.normalize(model_out["text_space_emb"],   dim=-1)
-                a_src= F.normalize(model_out["audio_source_emb"], dim=-1)
-                t_src= F.normalize(model_out["text_source_emb"],  dim=-1)
-                T = model_out["logit_scale"].exp()
-
-                loss_space  = sup_contrast(a_s,  t_s,  spa_lb, T)
-                loss_source = sup_contrast(a_src,t_src,src_lb, T)
-                physical_losses, total_physical_loss = physical_loss(model_out, batch_data, isNorm=False, dataloader=val_dl)
-
-                val_metrics["space"]    += loss_space.item()
-                val_metrics["source"]   += loss_source.item()
-                val_metrics["physical"] += total_physical_loss.item()
-                val_metrics["direction"] += physical_losses["loss_dir"].item()
-                val_metrics["distance"] += physical_losses["loss_distance"].item()
-                val_metrics["area"]     += physical_losses["loss_area"].item()
-                val_metrics["reverb"]   += physical_losses["loss_reverb"].item()
-                val_metrics["count"]    += 1
-
-        # ---- 平均を計算 ----
-        n = val_metrics["count"]
-        val_space    = val_metrics["space"]    / n
-        val_source   = val_metrics["source"]   / n
-        val_phys     = val_metrics["physical"] / n
-        val_direction = val_metrics["direction"] / n
-        val_distance = val_metrics["distance"] / n
-        val_area     = val_metrics["area"]     / n
-        val_reverb   = val_metrics["reverb"]   / n
-        val_mean     = (val_space + val_source + val_phys)
-
-        print(f"Epoch {ep}  [VAL] space={val_space:.4f} "
-            f"src={val_source:.4f} phys={val_phys:.4f}")
-
-        if cfg["wandb"]:
-            wandb.log({
-                "val/loss_space":    val_space,
-                "val/loss_source":   val_source,
-                "val/loss_physical": val_phys,
-                "val/loss_direction": val_direction,
-                "val/loss_distance": val_distance,
-                "val/loss_area":     val_area,  
-                "val/loss_reverb":   val_reverb,
-                "logit_scale":       model_out["logit_scale"].item(),
-                "val/loss_mean":     val_mean,
-                "epoch": ep
-            })
-
-        # -------- checkpoint -------------
-        ckpt_dir = Path("checkpoints"); ckpt_dir.mkdir(exist_ok=True)
-        torch.save({"model": model.state_dict(),
-                    "opt":   opt.state_dict(),
-                    "epoch": ep},
-                   ckpt_dir/f"ckpt_sup_ep{ep}.pt")
-        print(f"[✓] Saved checkpoint for epoch {ep}")
-
-
-if __name__ == "__main__":
-    try:
-        main()
-    finally:
-        if wandb.run is not None:
-            wandb.finish()
\ No newline at end of file
diff --git a/wandb/debug-internal.log b/wandb/debug-internal.log
index 89bf948..03c8713 120000
--- a/wandb/debug-internal.log
+++ b/wandb/debug-internal.log
@@ -1 +1 @@
-run-20250805_152347-y5i2arjj/logs/debug-internal.log
\ No newline at end of file
+run-20250812_173509-3vs81wec/logs/debug-internal.log
\ No newline at end of file
diff --git a/wandb/debug.log b/wandb/debug.log
index 3b1b88c..c336e74 120000
--- a/wandb/debug.log
+++ b/wandb/debug.log
@@ -1 +1 @@
-run-20250805_152347-y5i2arjj/logs/debug.log
\ No newline at end of file
+run-20250812_173509-3vs81wec/logs/debug.log
\ No newline at end of file
diff --git a/wandb/latest-run b/wandb/latest-run
index 1433bf2..04de0af 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20250805_152347-y5i2arjj
\ No newline at end of file
+run-20250812_173509-3vs81wec
\ No newline at end of file
