diff --git a/config.yaml b/config.yaml
index d091a8e..1d57484 100644
--- a/config.yaml
+++ b/config.yaml
@@ -4,12 +4,19 @@ audio_csv_train: AudioCaps_csv/train.csv
 rir_csv_train:   RIR_dataset/rir_catalog_train.csv
 
 # val
-audio_csv_val: AudioCaps_csv/val.csv        # 省略可
-rir_csv_val: RIR_dataset/rir_catalog_val.csv        # 省略可
+audio_csv_val: val_fixed_400x24/audio_fixed.csv      # 省略可
+rir_csv_val: val_fixed_400x24/rir_fixed.csv        # 省略可
 out_dir: /home/takamichi-lab-pc09/DELSA/Spatial_AudioCaps/takamichi09/for_delsa_spatialAudio 
 #予め作っておく クラスタ分散とMIG用
 SpatialAudio_csv_val: /home/takamichi-lab-pc09/DELSA/Spatial_AudioCaps/takamichi09/for_delsa_spatialAudio 
 
+# 前計算Valの場所（precompute_val.py の out_dir）
+val_precomp_root: Spatial_AudioCaps/takamichi09/for_delsa_spatialAudio
+val_index_csv:    Spatial_AudioCaps/takamichi09/for_delsa_spatialAudio/val_precomputed.csv   # 省略可（rootからの既定を使う）
+val_batch_size: 16
+val_on_start: true        # 起動直後にクイックVal
+val_every_steps: 0        # ステップ間クイックValを使うなら 200 等
+val_subset_batches: 2     # クイックValで使うValバッチ数
 audio_base: AudioCaps_mp3 
 batch_size: 8
 n_views: 2
diff --git a/dataset/__pycache__/audio_rir_dataset.cpython-312.pyc b/dataset/__pycache__/audio_rir_dataset.cpython-312.pyc
index c177722..1c4860c 100644
Binary files a/dataset/__pycache__/audio_rir_dataset.cpython-312.pyc and b/dataset/__pycache__/audio_rir_dataset.cpython-312.pyc differ
diff --git a/dataset/audio_rir_dataset.py b/dataset/audio_rir_dataset.py
index d6793d5..da00c50 100644
--- a/dataset/audio_rir_dataset.py
+++ b/dataset/audio_rir_dataset.py
@@ -161,7 +161,7 @@ class AudioRIRDataset(Dataset):
         Z =  (m0 -  m1 -  m2 +  m3)/2
         foa = torch.stack([W, Y, Z, X])
         #print(sr)
-        sf.write('foa.wav', foa.T, sr)
+        #sf.write('foa.wav', foa.T, sr)
         return foa
 
 
diff --git a/foa.wav b/foa.wav
index 1df7961..d97190c 100644
Binary files a/foa.wav and b/foa.wav differ
diff --git a/model/__pycache__/delsa_model.cpython-312.pyc b/model/__pycache__/delsa_model.cpython-312.pyc
index 9c9ddf6..a2c4f5b 100644
Binary files a/model/__pycache__/delsa_model.cpython-312.pyc and b/model/__pycache__/delsa_model.cpython-312.pyc differ
diff --git a/model/delsa_model.py b/model/delsa_model.py
index 7154414..e8bde49 100644
--- a/model/delsa_model.py
+++ b/model/delsa_model.py
@@ -1,4 +1,4 @@
-# file: models/delsa_model.py
+# file: model/delsa_model.py
 
 import torch
 import torch.nn as nn
diff --git a/precompute_val.py b/precompute_val.py
index c1c86ba..bb5f5be 100644
--- a/precompute_val.py
+++ b/precompute_val.py
@@ -1,44 +1,50 @@
+#!/usr/bin/env python3
 # scripts/precompute_val.py
-import argparse, json, torchaudio, torch, pandas as pd
-from pathlib import Path
-from dataset.audio_rir_dataset import AudioRIRDataset, foa_to_iv, rewrite_caption
-import torch 
-import torch.nn.functional as F
-from torch.utils.data import DataLoader
+# Dry×RIR を事前畳み込みして FOA/IV を保存し、val_precomputed.csv を作成
+# 進捗は tqdm の単一バーで表示（総ビュー数 = Audio件数 × n_views）
+
+import argparse
 from pathlib import Path
-import random
-import wandb
-import math
 import sys
+import random
 import yaml
-random.seed(42); torch.manual_seed(42)
-def _select_device(raw: str |None) -> str:
+import torch
+import torchaudio
+import pandas as pd
+from tqdm.auto import tqdm
+
+from dataset.audio_rir_dataset import AudioRIRDataset, foa_to_iv, rewrite_caption
+
+# 再現性
+random.seed(42)
+torch.manual_seed(42)
+
+def _select_device(raw: str | None) -> str:
     if raw is None or raw.lower() == "auto":
         return "cuda" if torch.cuda.is_available() else "cpu"
     return raw
 
 def load_config(path: str | None = None) -> dict:
-    """Load hyper‑parameters from a YAML file and fill in sane defaults."""
-
-    # 1️⃣  determine YAML path (CLI arg 1 or default "config.yaml")
+    """Load hyper-parameters from a YAML file and fill in sane defaults."""
+    # 1) YAML パス決定（CLIの第1引数が .yml/.yaml ならそれを優先）
     if path is None:
         if len(sys.argv) > 1 and sys.argv[1].endswith((".yml", ".yaml")):
             path = sys.argv[1]
         else:
             path = "config.yaml"
 
-    # 2️⃣  read YAML
+    # 2) 読み込み
     try:
-        with open(path, "r", encoding="utf‑8") as f:
+        with open(path, "r", encoding="utf-8") as f:
             cfg = yaml.safe_load(f) or {}
-    except FileNotFoundError as e:
-        raise SystemExit(f"[ERR] YAML not found: {path}") from e
+    except FileNotFoundError:
+        cfg = {}
 
-    # 3️⃣  defaults
+    # 3) デフォルト値
     defaults = {
         "split": "train",
         "batch_size": 8,
-        "n_views": 4,
+        "n_views": 24,  # 前計算で使う RIR 本数（<=0 なら全部）
         "epochs": 5,
         "lr": 0.0001,
         "device": "auto",
@@ -51,72 +57,124 @@ def load_config(path: str | None = None) -> dict:
         "rir_csv_val": "RIR_dataset/rir_catalog_val.csv",
         "base_dir": "RIR_dataset",
         "audio_base": None,
-
+        "out_dir": "data/val_precomputed",
     }
     for k, v in defaults.items():
         cfg.setdefault(k, v)
 
-    # 4️⃣  post‑process
+    # 4) 後処理
     cfg["device"] = _select_device(cfg["device"])
     return cfg
 
-def main(args):
+def main(args: argparse.Namespace):
     cfg = load_config()
-    if args.csv_audio: cfg["audio_csv_val"] = args.csv_audio
-    if args.csv_rir: cfg["rir_csv_val"] = args.csv_rir
-    if args.base_dir: cfg["audio_base"] = args.base_dir
-
+    # CLI で上書き
+    if args.csv_audio:
+        cfg["audio_csv_val"] = args.csv_audio
+    if args.csv_rir:
+        cfg["rir_csv_val"] = args.csv_rir
+    if args.base_dir:
+        cfg["audio_base"] = args.base_dir
     cfg["out_dir"] = args.out_dir or cfg.get("out_dir", "data/val_precomputed")
-    if args.n_views is not None:
-        cfg["n_views_val"] = args.n_views
 
+    # n_views（<=0 なら「全部」）
+    n_views_cfg = (
+        args.n_views
+        if args.n_views is not None
+        else cfg.get("val_n_views", cfg.get("n_views", 1))
+    )
+    # Dataset 準備（学習用の挙動は使わないが整合のため値は渡す）
     ds = AudioRIRDataset(
-        csv_audio=cfg["audio_csv_val"], base_dir=cfg["audio_base"],
-        csv_rir=cfg["rir_csv_val"],     split="val",
-        n_views=cfg["n_views_val"],     share_rir=False,  # ←ランダム性を排除
-        batch_size=None)
-    out_root = Path(cfg["out_dir"]); out_root.mkdir(parents=True, exist_ok=True)
-    records = []
-
+        csv_audio=cfg["audio_csv_val"],
+        base_dir=cfg["audio_base"],
+        csv_rir=cfg["rir_csv_val"],
+        split="val",
+        n_views=max(1, n_views_cfg if isinstance(n_views_cfg, int) else 1),
+        share_rir=False,
+        batch_size=None,
+    )
+
+    out_root = Path(cfg["out_dir"])
+    (out_root / "foa").mkdir(parents=True, exist_ok=True)
+    (out_root / "feat").mkdir(parents=True, exist_ok=True)
+
+    records: list[dict] = []
+
+    # RIR の決定論的順序
     rir_list = sorted(ds.rir_paths)
-    # n_views <=0を「全部のRIRを使う」と解釈
-    n_views = len(rir_list) if cfg["n_views"] <= 0 else min(cfg["n_views"],len(rir_list))
+    n_views = len(rir_list) if (isinstance(n_views_cfg, int) and n_views_cfg <= 0) else min(int(n_views_cfg), len(rir_list))
 
     torch.set_grad_enabled(False)
-    for idx in range(len(ds)):
-
-        row = ds.audio_df.iloc[idx]
-        dry   = ds._load_dry(row["audiocap_id"])
-        for rir_path in rir_list[:n_views]:
-            foa   = ds._apply_rir(dry, rir_path)            # 4-ch FOA:contentReference[oaicite:0]{index=0}
-            foa16 = torchaudio.functional.resample(foa, 48_000, 16_000)
-            # 1) タプル全体を受け取る
-            i_act, i_rea = foa_to_iv(foa16.unsqueeze(0))  # ← [0] を削除
-
-            # 2) バッチ次元 (B=1) を潰す
-            i_act, i_rea = i_act.squeeze(0), i_rea.squeeze(0)
-            # --- 保存 ---
-            key   = f"{row.audiocap_id}_{Path(rir_path).stem}"
-            (out_root / "foa").mkdir(exist_ok=True)
-            (out_root / "feat").mkdir(exist_ok=True)
-            torchaudio.save(out_root/f"foa/{key}.wav", foa, 48_000)
-            torch.save({'i_act': i_act, 'i_rea': i_rea}, out_root/f"feat/{key}.pt")
-
-            meta   = ds.rir_meta[rir_path].copy()
-            caption= rewrite_caption(row["caption"], meta)
-            records.append({
-                "audiocap_id": row.audiocap_id,
-                "rir_path": rir_path,
-                "foa_path": f"foa/{key}.wav",
-                "feat_path": f"feat/{key}.pt",
-                "caption": caption
-            })
-
-    pd.DataFrame(records).to_csv(out_root/"val_precomputed.csv", index=False)
+    total_views = len(ds) * n_views
+    # tqdm開始前に追加
+    print(f"[INFO] Audio clips={len(ds)} | RIR candidates={len(rir_list)} | "
+      f"n_views(effective)={n_views} | total views={len(ds)*n_views}")
+    pbar = tqdm(total=total_views, desc="Precomputing FOA/IV", unit="view")
+
+    with torch.no_grad():
+        for idx in range(len(ds)):
+            row = ds.audio_df.iloc[idx]
+            dry = ds._load_dry(row["audiocap_id"])  # (T,)
+            for j, rir_path in enumerate(rir_list[:n_views]):
+                # 4ch FOA を合成（48k）
+                foa = ds._apply_rir(dry, rir_path)  # (4, T_48k)
+
+                # FOA 48k を保存（16-bit PCM）
+                key = f"{row.audiocap_id}_{Path(rir_path).stem}"
+                torchaudio.save(
+                    out_root / f"foa/{key}.wav",
+                    foa,
+                    48_000,
+                    format="wav",
+                    encoding="PCM_S",
+                    bits_per_sample=16,
+                )
+
+                # FOA→16k へリサンプルして IV 特徴を作成
+                foa16 = torchaudio.functional.resample(foa, 48_000, 16_000)  # (4, T_16k)
+                i_act, i_rea = foa_to_iv(foa16.unsqueeze(0))  # (1,3,F,T) ×2
+                i_act, i_rea = i_act.squeeze(0), i_rea.squeeze(0)  # (3,F,T)
+
+                # 特徴保存
+                torch.save({"i_act": i_act, "i_rea": i_rea}, out_root / f"feat/{key}.pt")
+
+                # メタ→キャプション
+                meta = ds.rir_meta[rir_path].copy()
+                caption = rewrite_caption(row["caption"], meta)
+
+                records.append(
+                    {
+                        "audiocap_id": row.audiocap_id,
+                        "rir_path": rir_path,
+                        "foa_path": f"foa/{key}.wav",
+                        "feat_path": f"feat/{key}.pt",
+                        "caption": caption,
+                    }
+                )
+
+                # 進捗
+                if (j % 4) == 0:
+                    pbar.set_postfix({"clip": int(row["audiocap_id"]), "RIR": Path(rir_path).stem})
+                pbar.update(1)
+
+    pbar.close()
+
+    # 索引CSV
+    df = pd.DataFrame(records)
+    df.to_csv(out_root / "val_precomputed.csv", index=False)
+
+    # 簡易サマリ
+    print(f"[OK] Wrote {len(df)} views to {out_root.resolve()}")
+    print(f" - Audio clips: {len(ds)}")
+    print(f" - RIR used per clip: {n_views}")
+    print(f" - Output CSV: {out_root / 'val_precomputed.csv'}")
 
 if __name__ == "__main__":
     p = argparse.ArgumentParser()
-    p.add_argument("--csv_audio"); p.add_argument("--base_dir")
-    p.add_argument("--csv_rir");  p.add_argument("--out_dir")
-    p.add_argument("--n_views", type=int, help="<=0 なら RIR 全部を使用")
+    p.add_argument("--config", help="YAML path (default: config.yaml)")
+    p.add_argument("--csv_audio", help="Validation audio CSV (e.g., AudioCaps_csv/val.csv)")
+    p.add_argument("--base_dir", help="Base directory for audio files")
+    p.add_argument("--csv_rir", help="Validation RIR catalog CSV (must include 'rir_path')")
+    p.add_argument("--out_dir", help="Output root (default: data/val_precomputed)")
+    p.add_argument("--n_views", type=int, help="<=0 なら RIR 全部を使用（デフォルトは config.yaml の n_views）")
     main(p.parse_args())
diff --git a/train.py b/train.py
index db9318a..e407edb 100644
--- a/train.py
+++ b/train.py
@@ -1,33 +1,17 @@
 #!/usr/bin/env python3
-# train_sup.py  ― supervised contrastive (source / space) 版
-import torch 
-import torch.nn.functional as F
+# train.py  ― supervised contrastive (source / space) + per-epoch retrieval eval
+
+import torch, torch.nn.functional as F
 from torch.utils.data import DataLoader
 from pathlib import Path
-import random
-import wandb
-import math
-import sys
-import yaml
-
-
-
-
-# 容積の分類のloss
-# 音源の分類のloss 犬とか
-# adversarial loss 
-    # sourceの埋め込みからはspaceの埋め込みを予測できないようにする。
-    # 音源の分類は　spatialの埋め込みからはできないようにする。
-
-# 3週間くらいで結果揃える
-# ToDo: 正規化をする. 
+import random, wandb, math, sys, yaml
 
+from dataset.audio_rir_dataset import AudioRIRDataset, collate_fn   # 既存
+from dataset.precomputed_val_dataset import PrecomputedValDataset   # 追加①
+from utils.metrics import cosine_sim, recall_at_k                   # 追加②
+from model.delsa_model import DELSA                                # ← 修正
 
-random.seed(42)  # 再現性のため
-
-
-#ToDO:Supervised Contrasive Learningを読む。https://proceedings.neurips.cc/paper_files/paper/2020/file/d89a66c7c80a29b1bdbab0f2a1a94af8-Paper.pdf?utm_source=chatgpt.com
-# ────────────────────────── CLI ──────────────────────────
+random.seed(42)
 
 def _select_device(raw: str |None) -> str:
     if raw is None or raw.lower() == "auto":
@@ -35,23 +19,17 @@ def _select_device(raw: str |None) -> str:
     return raw
 
 def load_config(path: str | None = None) -> dict:
-    """Load hyper‑parameters from a YAML file and fill in sane defaults."""
-
-    # 1️⃣  determine YAML path (CLI arg 1 or default "config.yaml")
     if path is None:
         if len(sys.argv) > 1 and sys.argv[1].endswith((".yml", ".yaml")):
             path = sys.argv[1]
         else:
             path = "config.yaml"
-
-    # 2️⃣  read YAML
     try:
-        with open(path, "r", encoding="utf‑8") as f:
+        with open(path, "r", encoding="utf-8") as f:
             cfg = yaml.safe_load(f) or {}
     except FileNotFoundError as e:
         raise SystemExit(f"[ERR] YAML not found: {path}") from e
 
-    # 3️⃣  defaults
     defaults = {
         "split": "train",
         "batch_size": 8,
@@ -62,330 +40,244 @@ def load_config(path: str | None = None) -> dict:
         "wandb": True,
         "proj": "delsa-sup-contrast",
         "run_name": None,
+        # ↓ 追加（前計算Valの場所）
+        "val_precomp_root": None,   # 例: "Spatial_AudioCaps/takamichi09/for_delsa_spatialAudio"
+        "val_index_csv":    None,   # 例: 上記/root/val_precomputed.csv（未指定なら root を使う）
+        "val_batch_size":   16,
     }
+
     for k, v in defaults.items():
         cfg.setdefault(k, v)
-
-    # 4️⃣  post‑process
     cfg["device"] = _select_device(cfg["device"])
     return cfg
 
-
-
-
-
-# ───────── Supervised cross-modal contrastive los ─────────
-
-def sup_contrast(
-    a: torch.Tensor,
-    b: torch.Tensor,
-    labels: torch.Tensor,
-    logit_scale: torch.Tensor,
-    eps: float = 1e-8,
-    *,
-    symmetric: bool = True,      # True: Text↔Audio 両方向を平均
-    exclude_diag: bool = False   # True: 分母から自己相似を除外 (SCL 型)
-) -> torch.Tensor:
-    """
-    Cross‑modal Supervised Contrastive Loss (InfoNCE‑style).
-
-    Parameters
-    ----------
-    a, b        : [B, D]  L2‑normalized以外は何でも可。内部で normalize する。
-    labels      : [B]     同じ値を持つサンプルを正例集合とみなす。
-    logit_scale : ()      log(temperature^{-1})。CLIP と同じ扱い。
-    eps         : float   分母のゼロ除避け。
-    symmetric   : bool    True  -> (Text→Audio + Audio→Text)/2
-    exclude_diag: bool    True  -> 自己相似を分母から除外 (SCL 推奨)
-
-    Returns
-    -------
-    loss : torch.Tensor  scalar
-    """
-
-    # 1. L2‑normalize
-    a = F.normalize(a, dim=1)
-    b = F.normalize(b, dim=1)
-
-    # 2. temperature (clamp to avoid overflow)
+def sup_contrast(a, b, labels, logit_scale, eps=1e-8, *, symmetric=True, exclude_diag=False):
+    a = F.normalize(a, dim=1); b = F.normalize(b, dim=1)
     scale = torch.clamp(logit_scale, max=math.log(1e2)).exp()
-
-    # 3. logits  (Text→Audio)
-    logits_t2a = torch.matmul(a, b.T) * scale           # [B, B]
-
-    # optional: Audio→Text (reuse transpose for efficiency)
+    logits_t2a = (a @ b.T) * scale
     logits_a2t = logits_t2a.T if symmetric else None
 
-    def _directional_loss(logits: torch.Tensor) -> torch.Tensor:
+    def _dir_loss(logits):
         B = logits.size(0)
-
-        # --- positive mask ---
-        pos_mask = labels[:, None].eq(labels[None, :])          # [B, B] bool
-
-        # --- numerical stability (detach!) ---
+        pos_mask = labels[:, None].eq(labels[None, :])
         max_sim, _ = logits.max(dim=1, keepdim=True)
-        logits_stable = logits - max_sim.detach()               # stop‑grad on max
-
-        # --- denominator ---
-        if exclude_diag:
-            diag_mask = torch.eye(B, dtype=torch.bool, device=logits.device)
-        else:
-            diag_mask = torch.zeros(B, B, dtype=torch.bool, device=logits.device)
-
-        exp_sim = torch.exp(logits_stable) * (~diag_mask)
+        logits = logits - max_sim.detach()
+        diag_mask = torch.eye(B, dtype=torch.bool, device=logits.device) if exclude_diag else torch.zeros(B,B,dtype=torch.bool, device=logits.device)
+        exp_sim = torch.exp(logits) * (~diag_mask)
         denom = exp_sim.sum(dim=1, keepdim=True) + eps
-
-        # --- log‑probabilities ---
-        log_prob = logits_stable - denom.log()
+        log_prob = logits - denom.log()
         mean_log_pos = (log_prob * pos_mask.float()).sum(dim=1) / pos_mask.sum(dim=1).clamp(min=1)
-
-        # --- drop rows with no positives ---
         valid = pos_mask.any(dim=1)
         return -mean_log_pos[valid].mean()
 
-    loss_t2a = _directional_loss(logits_t2a)
+    loss_t2a = _dir_loss(logits_t2a)
     if symmetric:
-        loss_a2t = _directional_loss(logits_a2t)
-        return 0.5 * (loss_t2a + loss_a2t)
-    else:
-        return loss_t2a
-# ToDo;sup_contrastの公式実装見る：https://github.com/HobbitLong/SupContrast?utm_source=chatgpt.com
-
-
-# ── 物理量の予測損失 ──
-def physical_loss(model_output, batch_data,isNorm=True ,dataloader=None) -> torch.Tensor:
-    """
-    物理量の予測損失を計算する。
-    pred_physics: {
-        "direction": [B, 2]  # azimuth, elevation
-        "area": [B, 1]
-        "distance": [B, 1]
-        "reverb": [B, 1] """
+        return 0.5 * (loss_t2a + _dir_loss(logits_a2t))
+    return loss_t2a
+
+def physical_loss(model_output, batch_data, isNorm=True, dataloader=None,stats = None):
     if isNorm:
-        pred_physics = {
-            "direction": model_output["direction"],  
-            "area": model_output["area"],
-            "distance": model_output["distance"],
-            "reverb": model_output["reverb"]}  
-   
-        true_physics = {
-            "direction": batch_data["rir_meta"]["direction_vec"],  
-            "area": batch_data["rir_meta"]["area_m2_norm"],
-            "distance": batch_data["rir_meta"]["distance_norm"],
-            "reverb": batch_data["rir_meta"]["t30_norm"]} 
-        
-    else: #生の値で評価するとき
-        area_mean, area_std = dataloader.dataset.area_mean, dataloader.dataset.area_std
-        distance_mean, distance_std = dataloader.dataset.dist_mean, dataloader.dataset.dist_std
-        t30_mean, t30_std = dataloader.dataset.t30_mean, dataloader.dataset.t30_std
-        pred_physics = {
-            "direction": model_output["direction"],  
-            "area": model_output["area"] * area_std + area_mean,
-            "distance": model_output["distance"] * distance_std + distance_mean,
-            "reverb": model_output["reverb"] * t30_std + t30_mean
-        }
-        true_physics = {
-            "direction": batch_data["rir_meta"]["direction_vec"],
-            "area": batch_data["rir_meta"]["area_m2"],
-            "distance": batch_data["rir_meta"]["distance"],
-            "reverb": batch_data["rir_meta"]["fullband_T30_ms"]}    
-        
-    # 方向の損失は、cosine similarityを使う。
-    loss_dir = (-1* F.cosine_similarity(pred_physics["direction"], true_physics["direction"], dim=1)).mean()
-
-    #mse 自体がえ平均をとるので、平均をとる必要はない。
-    loss_area = F.mse_loss(pred_physics["area"].squeeze(-1), true_physics["area"])
-    loss_distance= F.mse_loss(pred_physics["distance"].squeeze(-1), true_physics["distance"])
-    loss_reverb = F.mse_loss(pred_physics["reverb"].squeeze(-1), true_physics["reverb"])
-
-    total_physical_loss = loss_dir + loss_area + loss_distance + loss_reverb
-    physical_losses = {
-        "total_loss": total_physical_loss, "loss_dir": loss_dir.item(),
-        "loss_distance": loss_distance.item(), "loss_area": loss_area.item(), "loss_reverb": loss_reverb.item()
+        pred = {"direction": model_output["direction"], "area": model_output["area"],
+                "distance": model_output["distance"], "reverb": model_output["reverb"]}
+        true = {"direction": batch_data["rir_meta"]["direction_vec"],
+                "area": batch_data["rir_meta"]["area_m2_norm"],
+                "distance": batch_data["rir_meta"]["distance_norm"],
+                "reverb": batch_data["rir_meta"]["t30_norm"]}
+    else:
+        area_mean, area_std = stats["area_mean"], stats["area_std"]
+        distance_mean, distance_std = stats["dist_mean"], stats["dist_std"]
+        t30_mean, t30_std = stats["t30_mean"], stats["t30_std"]
+        pred = {"direction": model_output["direction"],
+                "area": model_output["area"] * area_std + area_mean,
+                "distance": model_output["distance"] * distance_std + distance_mean,
+                "reverb": model_output["reverb"] * t30_std + t30_mean}
+        true = {"direction": batch_data["rir_meta"]["direction_vec"],
+                "area": batch_data["rir_meta"]["area_m2"],
+                "distance": batch_data["rir_meta"]["distance"],
+                "reverb": batch_data["rir_meta"]["fullband_T30_ms"]}
+    loss_dir = (-1 * F.cosine_similarity(pred["direction"], true["direction"], dim=1)).mean()
+    loss_area = F.mse_loss(pred["area"].squeeze(-1), true["area"])
+    loss_distance= F.mse_loss(pred["distance"].squeeze(-1), true["distance"])
+    loss_reverb = F.mse_loss(pred["reverb"].squeeze(-1), true["reverb"])
+    total = loss_dir + loss_area + loss_distance + loss_reverb
+    return {"loss_dir": loss_dir.item(),
+            "loss_distance": loss_distance.item(),
+            "loss_area": loss_area.item(),
+            "loss_reverb": loss_reverb.item()}, total
 
-    }
-    return physical_losses, total_physical_loss
-    
 def recursive_to(obj, device):
-    """Tensor だけを再帰的に device へ送る"""
-    if isinstance(obj, torch.Tensor):
-        return obj.to(device)
-    if isinstance(obj, dict):
-        return {k: recursive_to(v, device) for k, v in obj.items()}
-    if isinstance(obj, list):
-        return [recursive_to(v, device) for v in obj]
-    return obj          # int / float / str など
-# ─────────────────────────── Main ─────────────────────────
+    if isinstance(obj, torch.Tensor): return obj.to(device)
+    if isinstance(obj, dict):  return {k: recursive_to(v, device) for k,v in obj.items()}
+    if isinstance(obj, list):  return [recursive_to(v, device) for v in obj]
+    return obj
+
+@torch.no_grad()
+def eval_retrieval(model, loader, device, use_wandb=True, epoch=None):
+    """Source/Space 別の Retrieval を計算（view→group平均→R@K）。"""
+    model.eval()
+    bufs = {"t_spa":[], "a_spa":[], "t_src":[], "a_src":[], "ids_src":[], "ids_spa":[]}
+    for batch in loader:
+        if batch is None: continue
+        audio = {k: torch.stack([d[k] for d in batch["audio"]]).to(device)
+                 for k in ("i_act","i_rea","omni_48k")}
+        texts = batch["texts"]
+        out = model(audio, texts)
+        bufs["t_spa"].append(out["text_space_emb"].detach().cpu())
+        bufs["a_spa"].append(out["audio_space_emb"].detach().cpu())
+        bufs["t_src"].append(out["text_source_emb"].detach().cpu())
+        bufs["a_src"].append(out["audio_source_emb"].detach().cpu())
+        bufs["ids_src"].append(batch["source_id"].reshape(-1).cpu())
+        bufs["ids_spa"].append(batch["space_id"].reshape(-1).cpu())
+    for k in bufs: bufs[k] = torch.cat(bufs[k], dim=0)
+
+    def group_mean(emb, gids):
+        uniq = gids.unique(sorted=True)
+        pooled = torch.stack([emb[gids==u].mean(0) for u in uniq])
+        return pooled, uniq
+
+    t_src, id_src = group_mean(bufs["t_src"], bufs["ids_src"])
+    a_src, _      = group_mean(bufs["a_src"], bufs["ids_src"])
+    t_spa, id_spa = group_mean(bufs["t_spa"], bufs["ids_spa"])
+    a_spa, _      = group_mean(bufs["a_spa"], bufs["ids_spa"])
+
+    S_src = cosine_sim(a_src, t_src)  # [N_t, N_a]
+    S_spa = cosine_sim(a_spa, t_spa)
+
+    src_T2A = recall_at_k(S_src,   id_src, id_src, ks=(1,5,10))
+    src_A2T = recall_at_k(S_src.T, id_src, id_src, ks=(1,5,10))
+    spa_T2A = recall_at_k(S_spa,   id_spa, id_spa, ks=(1,5,10))
+    spa_A2T = recall_at_k(S_spa.T, id_spa, id_spa, ks=(1,5,10))
+
+    mets = {**{f"SRC/T2A/{k}":v for k,v in src_T2A.items()},
+            **{f"SRC/A2T/{k}":v for k,v in src_A2T.items()},
+            **{f"SPA/T2A/{k}":v for k,v in spa_T2A.items()},
+            **{f"SPA/A2T/{k}":v for k,v in spa_A2T.items()}}
+    if use_wandb: wandb.log({"epoch": epoch, **mets})
+    return mets
+
 def main():
-    cfg = load_config()  # config.yaml を読み込む
-    from dataset.audio_rir_dataset import AudioRIRDataset, collate_fn
+    cfg = load_config()
     if cfg["wandb"]:
-        wandb.init(project=cfg["proj"],
-                   name = cfg["run_name"],
-                   config = cfg, save_code =True,
-                   mode="online")
-    train_ds = AudioRIRDataset(
-        csv_audio=cfg["audio_csv_train"],
-        base_dir=cfg["audio_base"],
-        csv_rir=cfg["rir_csv_train"],
-        n_views=cfg["n_views"],
-        split=cfg["split"],
-        batch_size=cfg["batch_size"]
-    )
-    train_dl = DataLoader(train_ds, batch_size=cfg["batch_size"],
-                    shuffle=True, num_workers=4,
-                    collate_fn=collate_fn, pin_memory=False)
-    
-    # val 用  ★追加★
-    val_ds = AudioRIRDataset(
-        csv_audio=cfg["audio_csv_val"], base_dir=cfg["audio_base"],
-        csv_rir=cfg["rir_csv_val"], split="val",             # ここだけ split=val
-        n_views=cfg["n_views"],
-        batch_size=cfg["batch_size"])
-    val_dl = DataLoader(val_ds,  batch_size=cfg["batch_size"],
-                        shuffle=False, num_workers=4,
-                        collate_fn=collate_fn, pin_memory=False)
-    from model.delsa_model import DELSA      # アップロード済みモデル
+        wandb.init(project=cfg["proj"], name=cfg["run_name"], config=cfg, save_code=True, mode="online")
+
+    # -------- Train loader (従来通り) --------
+    train_ds = AudioRIRDataset(csv_audio=cfg["audio_csv_train"], base_dir=cfg["audio_base"],
+                               csv_rir=cfg["rir_csv_train"], n_views=cfg["n_views"],
+                               split=cfg["split"], batch_size=cfg["batch_size"])
+    train_dl = DataLoader(train_ds, batch_size=cfg["batch_size"], shuffle=True, num_workers=4,
+                          collate_fn=collate_fn, pin_memory=False)
+    train_stats = {
+        "area_mean": train_ds.area_mean, "area_std": train_ds.area_std,
+        "dist_mean": train_ds.dist_mean, "dist_std": train_ds.dist_std,
+        "t30_mean":  train_ds.t30_mean,  "t30_std":  train_ds.t30_std,
+    }
+    # -------- Val loader (前計算を読む) --------
+    val_root = cfg.get("val_precomp_root")
+    val_csv  = cfg.get("val_index_csv") or (str(Path(val_root)/"val_precomputed.csv") if val_root else None)
+    if not val_csv or not Path(val_csv).exists():
+        raise SystemExit(f"[ERR] val_precomputed.csv が見つかりません: {val_csv}")
+    val_ds = PrecomputedValDataset(index_csv=val_csv, rir_meta_csv=cfg["rir_csv_val"], root=val_root)
+    val_dl = DataLoader(val_ds, batch_size=cfg.get("val_batch_size", cfg["batch_size"]),
+                        shuffle=False, num_workers=4, collate_fn=collate_fn, pin_memory=False)
+
+    # -------- Model / Optim --------
     model = DELSA(audio_encoder_cfg={}, text_encoder_cfg={}).to(cfg["device"])
     opt   = torch.optim.AdamW(model.parameters(), lr=cfg["lr"])
-
     for ep in range(1, cfg["epochs"]+1):
         model.train()
         for step, batch in enumerate(train_dl, 1):
-            if batch is None:
-                continue
-            # ── データをデバイスに転送 ──
-            #batch_data = {k: v.to(cfg["device"]) for k, v in batch.items() if k not in ["audio", "texts"]}
-            batch_data = {k: recursive_to(v, cfg["device"])
-              for k, v in batch.items() if k not in ["audio","texts"]}
-            # ----------- flatten ------------
-
-            audio_dict = {k: torch.stack([d[k] for d in batch["audio"]]).to(cfg["device"])
-              for k in ("i_act","i_rea","omni_48k")}
+            if batch is None: continue
+            batch_data = {k: recursive_to(v, cfg["device"]) for k, v in batch.items() if k not in ["audio","texts"]}
+            audio = {k: torch.stack([d[k] for d in batch["audio"]]).to(cfg["device"]) for k in ("i_act","i_rea","omni_48k")}
             texts  = batch["texts"]
-            #print(texts)
-            src_lb = batch["source_id"].reshape(-1).to(cfg["device"])  # [B']
-            meta = batch["rir_meta"]  # RIRメタデータ
-            #print(meta)
+            src_lb = batch["source_id"].reshape(-1).to(cfg["device"])
             spa_lb = batch["space_id"].reshape(-1).to(cfg["device"])
-            #print(spa_lb)
-            # ----------- forward ------------
-            model_out = model(audio_dict, texts)   # expect 4 embeddings + logit_scale
 
-            a_s  = F.normalize(model_out["audio_space_emb"],  dim=-1)
-            t_s  = F.normalize(model_out["text_space_emb"],   dim=-1)
-            a_src= F.normalize(model_out["audio_source_emb"], dim=-1)
-            t_src= F.normalize(model_out["text_source_emb"],  dim=-1)
+            out = model(audio, texts)
+            a_s  = F.normalize(out["audio_space_emb"],  dim=-1)
+            t_s  = F.normalize(out["text_space_emb"],   dim=-1)
+            a_sr = F.normalize(out["audio_source_emb"], dim=-1)
+            t_sr = F.normalize(out["text_source_emb"],  dim=-1)
+            T = out["logit_scale"].exp()
 
-
-
-            T = model_out["logit_scale"].exp()     # 温度
             loss_space  = sup_contrast(a_s,  t_s,  spa_lb, T)
-            loss_source = sup_contrast(a_src,t_src,src_lb, T)
-
-            physical_losses, total_physical_loss = physical_loss(model_out, batch_data, isNorm=True, dataloader=train_dl)
+            loss_source = sup_contrast(a_sr, t_sr, src_lb, T)
+            phys_log, phys_loss = physical_loss(out, batch_data, isNorm=True, dataloader=train_dl)
 
-            loss = (loss_space + loss_source + total_physical_loss)
-
-            opt.zero_grad()
-            loss.backward()
-            opt.step()
+            loss = loss_space + loss_source + phys_loss
+            opt.zero_grad(); loss.backward(); opt.step()
 
             if step % 10 == 0:
-                print(f"Epoch {ep} Step {step}/{len(train_dl)}  "
-                      f"space={loss_space:.4f}  src={loss_source:.4f}")
+                print(f"Epoch {ep} Step {step}/{len(train_dl)}  space={loss_space:.4f}  src={loss_source:.4f}")
                 if cfg["wandb"]:
-                    wandb.log({
-                        "loss/space": loss_space.item(),
-                        "loss/source": loss_source.item(),
-                        "loss/physical": total_physical_loss.item(),
-                        "loss/dir": physical_losses["loss_dir"],
-                        "loss/distance": physical_losses["loss_distance"],
-                        "loss/area": physical_losses["loss_area"],  
-                        "loss/reverb": physical_losses["loss_reverb"],
-                        "logit_scale": model_out["logit_scale"].item(),
-                        "loss/mean": loss.item(),
-                        "epoch": ep,
-                        "step": step + (ep-1)*len(train_dl)
-                    })
-        # ---------- Validation ----------  ★追加ブロック★
+                    wandb.log({"loss/space": loss_space.item(), "loss/source": loss_source.item(),
+                               "loss/physical": phys_loss.item(), "loss/dir": phys_log["loss_dir"],
+                               "loss/distance": phys_log["loss_distance"], "loss/area": phys_log["loss_area"],
+                               "loss/reverb": phys_log["loss_reverb"], "logit_scale": out["logit_scale"].item(),
+                               "loss/mean": loss.item(), "epoch": ep, "step": step + (ep-1)*len(train_dl)})
+
+        # -------- Validation (Retrieval + 物理lossの平均) --------
         model.eval()
-        val_metrics = {"space": 0.0, "source": 0.0,
-                    "physical": 0.0, "count": 0}
+        val_losses = {"space":0.0,"source":0.0,"physical":0.0,"direction":0.0,"distance":0.0,"area":0.0,"reverb":0.0,"count":0}
+
         with torch.no_grad():
             for batch in val_dl:
-                if batch is None:  # collate_fn が None を返すケース対策
-                    continue
-                # --- デバイス転送／forward は train ループと同じ ---
-                batch_data = {k: v.to(cfg["device"]) for k, v in batch.items()
-                            if k not in ["audio","texts"]}
-                audio_dict = {k: torch.stack([d[k] for d in batch["audio"]]).to(args.device)
-                            for k in ("i_act","i_rea","omni_48k")}
+                if batch is None: continue
+                batch_data = {k: recursive_to(v, cfg["device"]) for k, v in batch.items() if k not in ["audio","texts"]}
+                audio = {k: torch.stack([d[k] for d in batch["audio"]]).to(cfg["device"]) for k in ("i_act","i_rea","omni_48k")}
                 texts  = batch["texts"]
                 src_lb = batch["source_id"].reshape(-1).to(cfg["device"])
                 spa_lb = batch["space_id"].reshape(-1).to(cfg["device"])
 
-                model_out = model(audio_dict, texts)
-
-                a_s  = F.normalize(model_out["audio_space_emb"],  dim=-1)
-                t_s  = F.normalize(model_out["text_space_emb"],   dim=-1)
-                a_src= F.normalize(model_out["audio_source_emb"], dim=-1)
-                t_src= F.normalize(model_out["text_source_emb"],  dim=-1)
-                T = model_out["logit_scale"].exp()
-
-                loss_space  = sup_contrast(a_s,  t_s,  spa_lb, T)
-                loss_source = sup_contrast(a_src,t_src,src_lb, T)
-                physical_losses, total_physical_loss = physical_loss(model_out, batch_data, isNorm=False, dataloader=val_dl)
-
-                val_metrics["space"]    += loss_space.item()
-                val_metrics["source"]   += loss_source.item()
-                val_metrics["physical"] += total_physical_loss.item()
-                val_metrics["direction"] += physical_losses["loss_dir"].item()
-                val_metrics["distance"] += physical_losses["loss_distance"].item()
-                val_metrics["area"]     += physical_losses["loss_area"].item()
-                val_metrics["reverb"]   += physical_losses["loss_reverb"].item()
-                val_metrics["count"]    += 1
-
-        # ---- 平均を計算 ----
-        n = val_metrics["count"]
-        val_space    = val_metrics["space"]    / n
-        val_source   = val_metrics["source"]   / n
-        val_phys     = val_metrics["physical"] / n
-        val_direction = val_metrics["direction"] / n
-        val_distance = val_metrics["distance"] / n
-        val_area     = val_metrics["area"]     / n
-        val_reverb   = val_metrics["reverb"]   / n
-        val_mean     = (val_space + val_source + val_phys)
-
-        print(f"Epoch {ep}  [VAL] space={val_space:.4f} "
-            f"src={val_source:.4f} phys={val_phys:.4f}")
-
+                out = model(audio, texts)
+                a_s  = F.normalize(out["audio_space_emb"],  dim=-1)
+                t_s  = F.normalize(out["text_space_emb"],   dim=-1)
+                a_sr = F.normalize(out["audio_source_emb"], dim=-1)
+                t_sr = F.normalize(out["text_source_emb"],  dim=-1)
+                T = out["logit_scale"].exp()
+
+                l_sp  = sup_contrast(a_s,  t_s,  spa_lb, T)
+                l_sr  = sup_contrast(a_sr, t_sr, src_lb, T)
+                phys_log, l_phys = physical_loss(out, batch_data, isNorm=False, dataloader=val_dl, stats=train_stats)
+
+                val_losses["space"]    += l_sp.item()
+                val_losses["source"]   += l_sr.item()
+                val_losses["physical"] += l_phys.item()
+                val_losses["direction"]+= phys_log["loss_dir"]
+                val_losses["distance"] += phys_log["loss_distance"]
+                val_losses["area"]     += phys_log["loss_area"]
+                val_losses["reverb"]   += phys_log["loss_reverb"]
+                val_losses["count"]    += 1
+
+        n = max(1, val_losses["count"])
+        val_mean = (val_losses["space"]/n + val_losses["source"]/n + val_losses["physical"]/n)
+        print(f"Epoch {ep}  [VAL] space={val_losses['space']/n:.4f}  src={val_losses['source']/n:.4f}  phys={val_losses['physical']/n:.4f}")
+
+        # Retrieval（Source/Space 別）
+        mets = eval_retrieval(model, val_dl, cfg["device"], use_wandb=cfg.get("wandb", False), epoch=ep)
         if cfg["wandb"]:
             wandb.log({
-                "val/loss_space":    val_space,
-                "val/loss_source":   val_source,
-                "val/loss_physical": val_phys,
-                "val/loss_direction": val_direction,
-                "val/loss_distance": val_distance,
-                "val/loss_area":     val_area,  
-                "val/loss_reverb":   val_reverb,
-                "logit_scale":       model_out["logit_scale"].item(),
+                "val/loss_space": val_losses["space"]/n,
+                "val/loss_source": val_losses["source"]/n,
+                "val/loss_physical": val_losses["physical"]/n,
+                "val/loss_direction": val_losses["direction"]/n,
+                "val/loss_distance": val_losses["distance"]/n,
+                "val/loss_area":     val_losses["area"]/n,
+                "val/loss_reverb":   val_losses["reverb"]/n,
                 "val/loss_mean":     val_mean,
-                "epoch": ep
+                "epoch": ep, **{f"val/{k}": v for k,v in mets.items()}
             })
 
         # -------- checkpoint -------------
         ckpt_dir = Path("checkpoints"); ckpt_dir.mkdir(exist_ok=True)
-        torch.save({"model": model.state_dict(),
-                    "opt":   opt.state_dict(),
-                    "epoch": ep},
+        torch.save({"model": model.state_dict(), "opt": opt.state_dict(), "epoch": ep},
                    ckpt_dir/f"ckpt_sup_ep{ep}.pt")
         print(f"[✓] Saved checkpoint for epoch {ep}")
 
-
 if __name__ == "__main__":
     try:
         main()
     finally:
         if wandb.run is not None:
-            wandb.finish()
\ No newline at end of file
+            wandb.finish()
+        print("[✓] Finished training.")
\ No newline at end of file
diff --git a/wandb/debug-internal.log b/wandb/debug-internal.log
index 89bf948..6f2ca27 120000
--- a/wandb/debug-internal.log
+++ b/wandb/debug-internal.log
@@ -1 +1 @@
-run-20250805_152347-y5i2arjj/logs/debug-internal.log
\ No newline at end of file
+run-20250812_150450-mu3mdm7e/logs/debug-internal.log
\ No newline at end of file
diff --git a/wandb/debug.log b/wandb/debug.log
index 3b1b88c..12c4c8e 120000
--- a/wandb/debug.log
+++ b/wandb/debug.log
@@ -1 +1 @@
-run-20250805_152347-y5i2arjj/logs/debug.log
\ No newline at end of file
+run-20250812_150450-mu3mdm7e/logs/debug.log
\ No newline at end of file
diff --git a/wandb/latest-run b/wandb/latest-run
index 1433bf2..d0fa410 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20250805_152347-y5i2arjj
\ No newline at end of file
+run-20250812_150450-mu3mdm7e
\ No newline at end of file
